{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming for Data Science and Artificial Intelligence\n",
    "\n",
    "## 0. Linear Regression Extension\n",
    "\n",
    "You have learned all the detail of Linear Regression. Its core idea is presented in the lecture.\n",
    "\n",
    "To enhance the `Gradient` function, there are two extensions that improve the optimization process in terms of quality of life (your life).\n",
    "\n",
    "1. Early Stopping\n",
    "2. Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Code from the class\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "def mean_squared_error(ytrue, ypred):\n",
    "    return ((ypred - ytrue) ** 2).sum() / ytrue.shape[0]\n",
    "\n",
    "class LinearRegression(object):\n",
    "    \n",
    "    kfold = KFold(n_splits=5)\n",
    "            \n",
    "    def __init__(self, alpha=0.001, num_epochs=5, batch_size=50, method='batch',\n",
    "                 cv=kfold):\n",
    "        self.alpha      = alpha\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.method     = method\n",
    "        self.cv         = cv\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        \n",
    "        #using training......\n",
    "        \n",
    "        #please change it to cross-validation.....\n",
    "        \n",
    "        #create a list of kfold scores\n",
    "        self.kfold = list()\n",
    "\n",
    "        #Kfold.split in the sklearn.....\n",
    "        #5 splits\n",
    "        for fold, (train_idx, val_idx) in enumerate(self.cv.split(X_train)):\n",
    "            \n",
    "            X_cross_train = X_train[train_idx]\n",
    "            y_cross_train = y_train[train_idx]\n",
    "            X_cross_val   = X_train[val_idx]\n",
    "            y_cross_val   = y_train[val_idx]\n",
    "            \n",
    "            #create self.theta here\n",
    "            self.theta = np.zeros(X_cross_train.shape[1])\n",
    "            \n",
    "            #define X_cross_train as only a subset of the data\n",
    "            #how big is this subset?  => mini-batch size ==> 50\n",
    "            \n",
    "            #one epoch will exhaust the WHOLE training set\n",
    "            for epoch in range(self.num_epochs):\n",
    "            \n",
    "                #with replacement or no replacement\n",
    "                #with replacement means just randomize\n",
    "                #with no replacement means 0:50, 51:100, 101:150, ......300:323\n",
    "                #shuffle your index\n",
    "                #===> please shuffle your index\n",
    "                perm = np.random.permutation(X_cross_train.shape[0])\n",
    "                        \n",
    "                X_cross_train = X_cross_train[perm]\n",
    "                y_cross_train = y_cross_train[perm]\n",
    "                \n",
    "                if   self.method == 'sto':\n",
    "                    for batch_idx in range(X_cross_train.shape[0]):\n",
    "                        X_method_train = X_cross_train[batch_idx].reshape(1, -1) #(11,) ==> (1, 11) ==> (m, n)\n",
    "                        y_method_train = y_cross_train[batch_idx]                    \n",
    "                        self._train(X_method_train, y_method_train)\n",
    "                elif self.method == 'mini':\n",
    "                    for batch_idx in range(0, X_cross_train.shape[0], self.batch_size):\n",
    "                        #batch_idx = 0, 50, 100, 150\n",
    "                        X_method_train = X_cross_train[batch_idx:batch_idx+self.batch_size, :]\n",
    "                        y_method_train = y_cross_train[batch_idx:batch_idx+self.batch_size]\n",
    "                        self._train(X_method_train, y_method_train)\n",
    "                else:\n",
    "                    X_method_train = X_cross_train\n",
    "                    y_method_train = y_cross_train\n",
    "                    self._train(X_method_train, y_method_train)\n",
    "\n",
    "                ####### Early stopping #######\n",
    "                loss = mean_squared_error(self.predict(X_cross_train), y_cross_train)\n",
    "                if(loss < threshold): # Find your own way to decalre threshold\n",
    "                    break\n",
    "                ##############################\n",
    "                    \n",
    "            yhat_val = self.predict(X_cross_val)\n",
    "            self.kfold.append(mean_squared_error(y_cross_val, yhat_val))\n",
    "            print(f\"Fold {fold}: {mean_squared_error(y_cross_val, yhat_val)}\")\n",
    "                    \n",
    "    def _train(self, X, y):\n",
    "        yhat = self.predict(X)\n",
    "        grad = X.T @(yhat - y)\n",
    "\n",
    "        ####### Momentum #######\n",
    "        # Find your own way to declare momentum and prev_step\n",
    "        step = self.alpha * grad\n",
    "        # momentum -> range from 0 - 1\n",
    "        new_move = step + (momentum * prev_step)\n",
    "        self.theta = self.theta - new_move\n",
    "        prev_step = step\n",
    "        ########################\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return X @ self.theta  #===>(m, n) @ (n, )\n",
    "    \n",
    "    def _coef(self):\n",
    "        return self.theta[1:]  #remind that theta is (w0, w1, w2, w3, w4.....wn)\n",
    "                               #w0 is the bias or the intercep\n",
    "                               #w1....wn are the weights / coefficients / theta\n",
    "        \n",
    "    def _bias(self):\n",
    "        return self.theta[0]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Early Stopping\n",
    "\n",
    "`num_epochs` is the parameter that specify how many iteration your model will perform the optimization.\n",
    "\n",
    "The question to ask is \"How many `num_epochs` is enough?\".\n",
    "\n",
    "The answer is when your model is optimized.\n",
    "\n",
    "Optimized? What is optimized?\n",
    "\n",
    "Ah.. young padawan, remember this figure?\n",
    "\n",
    "<img width=\"400\" src = \"figures/gradient.png\">\n",
    "\n",
    "We say the model is trained/optimized/learned when *it* reachs the minima, preferably, global minima.\n",
    "\n",
    "Our goal of optimization in this Gradient context is to minimize the loss/error.\n",
    "\n",
    "$$ \\min_{\\theta} || \\hat{y} - y ||^2_2 $$\n",
    "\n",
    "$\\theta$ is vector of weights to optimize.\n",
    "\n",
    "$\\hat{y}$ is actually a result of $ \\mathbf{X}\\theta $.\n",
    "\n",
    "$y$ is a vector of ground-truth/label.\n",
    "\n",
    "$|| . ||^2_2$ is the loss/error function. \n",
    "\n",
    "Now, you remember your objective. It is to minimize the $|| . ||^2_2$ by adjusting the $ \\theta $\n",
    "\n",
    "Then, the model is done training when you finally get the $ \\theta $ that yeild the best result (lowest error / minima).\n",
    "\n",
    "### How do you know you are there at the minima?\n",
    "\n",
    "Well, one way is to tracking your loss over the training session. \n",
    "\n",
    "![loss](https://miro.medium.com/max/411/1*UHmMMH3OhrBvgw18jKiy6Q.png)\n",
    "\n",
    "You see, when you plot the loss/error (in the figure is cost) over the epochs, it should be desending and finally plateau.\n",
    "\n",
    "When it is plateau, in other words, adjusting $\\theta$ no longer improve the prediction. You are at the minima. (Gradient is 0 at minima).\n",
    "\n",
    "According to the plot of lost/error, when do we reach the minima? 3000?, 2000?, 1000?\n",
    "\n",
    "In this case, do you need to set `num_epochs` to 3000?\n",
    "\n",
    "In the next dataset you will work on, what `num_epochs` you need to set?\n",
    "\n",
    "Ah.. You will never know until you train the model with that new dataset, at least, one time.\n",
    "\n",
    "Can we do better?\n",
    "\n",
    "Yes, we can. Introducing!!!!!!!!! `Early Stopping`!!!!!!!!!\n",
    "\n",
    "### Early Stopping\n",
    "\n",
    "The idea of `early stoping` is, as the name suggest, break the training before it reachs the `num_epochs`.\n",
    "\n",
    "The key is **no longer improve**. **Improve!!**. What is improve?\n",
    "\n",
    "Improve is the loss of iteration/epoch *i* is lower than the *i-1*.\n",
    "\n",
    "How much is the $\\text{loss}_i - \\text{loss}_{i-1}$ so that we say it is improving? This is up to you. \n",
    "\n",
    "Now code!!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### pesudo code for people who has no idea\n",
    "\n",
    "# for _ in range(epochs)\n",
    "#   train(x_train, y_train)\n",
    "#   loss = error(predict(x_train) - y_train)\n",
    "#   if prev_loss - loss > theshold\n",
    "#      ok, there is an improment\n",
    "#      prev_loss = loss\n",
    "#   else\n",
    "#      done training. break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Momentum\n",
    "\n",
    "`alpha` or `learning_rate` is the parameter how big of the step you will take on each Gradient optimization step.\n",
    "\n",
    "![overshoot](https://miro.medium.com/max/700/1*hGhRddOUV8h0pdQek8T35A.png)\n",
    "\n",
    "How much is `alpha` you should set?\n",
    "\n",
    "Not big such that it causes the *over shooting*.\n",
    "\n",
    "And certainly, not too small such that it causes you extra epochs to reach the minima.\n",
    "\n",
    "You want **just right** `alpha`.\n",
    "\n",
    "How do you know what is the right `alpha`?\n",
    "\n",
    "You guess, right?\n",
    "\n",
    "This time, 0.1. Observe the loss. If it is overshooting, stop and reduce the `alpha`. If the loss reduce slightly, maybe stop and increase the `alpha`.\n",
    "\n",
    "Finally, if you spend time adjusting the `alpha`, you will eventually find the **just right** `alpha`.\n",
    "\n",
    "Can we do better? Can we reduce the number of epochs without searching for the **just right** `alpha`.\n",
    "\n",
    "Yes, we can!!! Introducing!!!!!!!! MOMENTUM~~!!!!!!!!!\n",
    "\n",
    "### Momentum\n",
    "\n",
    "Here is the updating equation.\n",
    "\n",
    "$$ \\theta^{i+1} = \\theta^{i} - \\alpha \\times \\text{gradient}^i $$\n",
    "\n",
    "$i$ is the iteration/epoch number.\n",
    "\n",
    "$\\theta^{i+1}$ is the updated weights and will be used for the next iteration.\n",
    "\n",
    "$\\theta^{i}$ is the current weights of iteration $i$.\n",
    "\n",
    "$\\alpha$ is the learning rate.\n",
    "\n",
    "$\\text{gradient}^i$ is the step suggesting by the deviation of loss function from iteration $i$.\n",
    "\n",
    "$\\alpha \\times \\text{gradient}^i$ can be called *step*. When consider $i$, we call the $step^i$ how big of the step you take in iteration $i$.\n",
    "\n",
    "We know, in the early iteration, the step will be very big. Given the `alpha` is not too big, the step will decrase over time.\n",
    "\n",
    "Let's say our step size was big in the previous iteration (i-1), maybe, instead of taking $step^i$, we can take bigger step than $step^i$.\n",
    "\n",
    "Yes, and here is the equation.\n",
    "\n",
    "$$ \\theta^{i+1} = \\theta^{i} - step^i + \\text{momentum} \\times step^{i-1} $$\n",
    "\n",
    "Then, *momentum* here is how much do you want to include the information of previous step. The range of *momentum* is $[0.0 - 1.0]$. When *momentum* is 0 then you do not use previous step (basically, normal optimization). When it is 1 then you fully use the information from previous step. \n",
    "\n",
    "Now, code !!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### pesudo code for people who has no idea\n",
    "\n",
    "# step = alpha * grad\n",
    "# theta = theta - step + momentum * prev_step\n",
    "# prev_step = step"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
