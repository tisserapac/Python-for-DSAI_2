{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming for Data Science and Artificial Intelligence\n",
    "\n",
    "## 0. Linear Regression Extension\n",
    "\n",
    "You have learned all the detail of Linear Regression. Its core idea is presented in the lecture.\n",
    "\n",
    "To enhance the `Gradient` function, there are two extensions that improve the optimization process in terms of quality of life (your life).\n",
    "\n",
    "1. Early Stopping\n",
    "2. Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Code from the class\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "def mean_squared_error(ytrue, ypred):\n",
    "    return ((ypred - ytrue) ** 2).sum() / ytrue.shape[0]\n",
    "\n",
    "class LinearRegression_v1(object):\n",
    "    \n",
    "    kfold = KFold(n_splits=5)\n",
    "            \n",
    "    def __init__(self, alpha=0.001, num_epochs=5, batch_size=50, method='batch',\n",
    "                 cv=kfold):\n",
    "        self.alpha      = alpha\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.method     = method\n",
    "        self.cv         = cv\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        \n",
    "        #using training......\n",
    "        \n",
    "        #please change it to cross-validation.....\n",
    "        \n",
    "        #create a list of kfold scores\n",
    "        self.kfold = list()\n",
    "\n",
    "        #Kfold.split in the sklearn.....\n",
    "        #5 splits\n",
    "        for fold, (train_idx, val_idx) in enumerate(self.cv.split(X_train)):\n",
    "            \n",
    "            X_cross_train = X_train[train_idx]\n",
    "            y_cross_train = y_train[train_idx]\n",
    "            X_cross_val   = X_train[val_idx]\n",
    "            y_cross_val   = y_train[val_idx]\n",
    "            \n",
    "            #create self.theta here\n",
    "            self.theta = np.zeros(X_cross_train.shape[1])\n",
    "            \n",
    "            #define X_cross_train as only a subset of the data\n",
    "            #how big is this subset?  => mini-batch size ==> 50\n",
    "            \n",
    "            #one epoch will exhaust the WHOLE training set\n",
    "            for epoch in range(self.num_epochs):\n",
    "            \n",
    "                #with replacement or no replacement\n",
    "                #with replacement means just randomize\n",
    "                #with no replacement means 0:50, 51:100, 101:150, ......300:323\n",
    "                #shuffle your index\n",
    "                #===> please shuffle your index\n",
    "                perm = np.random.permutation(X_cross_train.shape[0])\n",
    "                        \n",
    "                X_cross_train = X_cross_train[perm]\n",
    "                y_cross_train = y_cross_train[perm]\n",
    "                \n",
    "                if   self.method == 'sto':\n",
    "                    for batch_idx in range(X_cross_train.shape[0]):\n",
    "                        X_method_train = X_cross_train[batch_idx].reshape(1, -1) #(11,) ==> (1, 11) ==> (m, n)\n",
    "                        y_method_train = y_cross_train[batch_idx]                    \n",
    "                        self._train(X_method_train, y_method_train)\n",
    "                elif self.method == 'mini':\n",
    "                    for batch_idx in range(0, X_cross_train.shape[0], self.batch_size):\n",
    "                        #batch_idx = 0, 50, 100, 150\n",
    "                        X_method_train = X_cross_train[batch_idx:batch_idx+self.batch_size, :]\n",
    "                        y_method_train = y_cross_train[batch_idx:batch_idx+self.batch_size]\n",
    "                        self._train(X_method_train, y_method_train)\n",
    "                else:\n",
    "                    X_method_train = X_cross_train\n",
    "                    y_method_train = y_cross_train\n",
    "                    self._train(X_method_train, y_method_train)\n",
    "                    \n",
    "            yhat_val = self.predict(X_cross_val)\n",
    "            self.kfold.append(mean_squared_error(y_cross_val, yhat_val))\n",
    "            print(f\"Fold {fold}: {mean_squared_error(y_cross_val, yhat_val)}\")\n",
    "                    \n",
    "    def _train(self, X, y):\n",
    "        yhat = self.predict(X)\n",
    "        grad = X.T @(yhat - y)\n",
    "        self.theta = self.theta - self.alpha * grad\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return X @ self.theta  #===>(m, n) @ (n, )\n",
    "    \n",
    "    def _coef(self):\n",
    "        return self.theta[1:]  #remind that theta is (w0, w1, w2, w3, w4.....wn)\n",
    "                               #w0 is the bias or the intercep\n",
    "                               #w1....wn are the weights / coefficients / theta\n",
    "        \n",
    "    def _bias(self):\n",
    "        return self.theta[0]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Early Stopping\n",
    "\n",
    "`num_epochs` is the parameter that specify how many iteration your model will perform the optimization.\n",
    "\n",
    "The question to ask is \"How many `num_epochs` is enough?\".\n",
    "\n",
    "The answer is when your model is optimized.\n",
    "\n",
    "Optimized? What is optimized?\n",
    "\n",
    "Ah.. young padawan, remember this figure?\n",
    "\n",
    "<img width=\"400\" src = \"figures/gradient.png\">\n",
    "\n",
    "We say the model is trained/optimized/learned when *it* reachs the minima, preferably, global minima.\n",
    "\n",
    "Our goal of optimization in this Gradient context is to minimize the loss/error.\n",
    "\n",
    "$$ \\min_{\\theta} || \\hat{y} - y ||^2_2 $$\n",
    "\n",
    "$\\theta$ is vector of weights to optimize.\n",
    "\n",
    "$\\hat{y}$ is actually a result of $ \\mathbf{X}\\theta $.\n",
    "\n",
    "$y$ is a vector of ground-truth/label.\n",
    "\n",
    "$|| . ||^2_2$ is the loss/error function. \n",
    "\n",
    "Now, you remember your objective. It is to minimize the $|| . ||^2_2$ by adjusting the $ \\theta $\n",
    "\n",
    "Then, the model is done training when you finally get the $ \\theta $ that yeild the best result (lowest error / minima).\n",
    "\n",
    "### How do you know you are there at the minima?\n",
    "\n",
    "Well, one way is to tracking your loss over the training session. \n",
    "\n",
    "![loss](https://miro.medium.com/max/411/1*UHmMMH3OhrBvgw18jKiy6Q.png)\n",
    "\n",
    "You see, when you plot the loss/error (in the figure is cost) over the epochs, it should be desending and finally plateau.\n",
    "\n",
    "When it is plateau, in other words, adjusting $\\theta$ no longer improve the prediction. You are at the minima. (Gradient is 0 at minima).\n",
    "\n",
    "According to the plot of lost/error, when do we reach the minima? 3000?, 2000?, 1000?\n",
    "\n",
    "In this case, do you need to set `num_epochs` to 3000?\n",
    "\n",
    "In the next dataset you will work on, what `num_epochs` you need to set?\n",
    "\n",
    "Ah.. You will never know until you train the model with that new dataset, at least, one time.\n",
    "\n",
    "Can we do better?\n",
    "\n",
    "Yes, we can. Introducing!!!!!!!!! `Early Stopping`!!!!!!!!!\n",
    "\n",
    "### Early Stopping\n",
    "\n",
    "The idea of `early stoping` is, as the name suggest, break the training before it reachs the `num_epochs`.\n",
    "\n",
    "The key is **no longer improve**. **Improve!!**. What is improve?\n",
    "\n",
    "Improve is the loss of iteration/epoch *i* is lower than the *i-1*.\n",
    "\n",
    "How much is the $\\text{loss}_i - \\text{loss}_{i-1}$ so that we say it is improving? This is up to you. \n",
    "\n",
    "Now code!!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### pesudo code for people who has no idea\n",
    "\n",
    "# for _ in range(epochs)\n",
    "#   train(x_train, y_train)\n",
    "#   loss = error(predict(x_train) - y_train)\n",
    "#   if prev_loss - loss > theshold\n",
    "#      ok, there is an improment\n",
    "#      prev_loss = loss\n",
    "#   else\n",
    "#      done training. break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "def mean_squared_error(ytrue, ypred):\n",
    "    return ((ypred - ytrue) ** 2).sum() / ytrue.shape[0]\n",
    "\n",
    "class LinearRegression_v2(object):\n",
    "    \n",
    "    kfold = KFold(n_splits=5)\n",
    "            \n",
    "    def __init__(self, alpha=0.001, num_epochs=5, batch_size=50, method='batch',\n",
    "                 cv=kfold, theshold=0.000001):\n",
    "        self.alpha      = alpha\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.method     = method\n",
    "        self.cv         = cv\n",
    "        self.theshold   = theshold\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        \n",
    "        #using training......\n",
    "        \n",
    "        #please change it to cross-validation.....\n",
    "        \n",
    "        #create a list of kfold scores\n",
    "        self.kfold = list()\n",
    "\n",
    "        #Kfold.split in the sklearn.....\n",
    "        #5 splits\n",
    "        for fold, (train_idx, val_idx) in enumerate(self.cv.split(X_train)):\n",
    "            \n",
    "            X_cross_train = X_train[train_idx]\n",
    "            y_cross_train = y_train[train_idx]\n",
    "            X_cross_val   = X_train[val_idx]\n",
    "            y_cross_val   = y_train[val_idx]\n",
    "            \n",
    "            #create self.theta here\n",
    "            self.theta = np.zeros(X_cross_train.shape[1])\n",
    "            \n",
    "            #define X_cross_train as only a subset of the data\n",
    "            #how big is this subset?  => mini-batch size ==> 50\n",
    "\n",
    "            #Set previous loss to 0\n",
    "            prev_loss = 0\n",
    "            \n",
    "            #one epoch will exhaust the WHOLE training set\n",
    "            for epoch in range(self.num_epochs):\n",
    "            \n",
    "                #with replacement or no replacement\n",
    "                #with replacement means just randomize\n",
    "                #with no replacement means 0:50, 51:100, 101:150, ......300:323\n",
    "                #shuffle your index\n",
    "                #===> please shuffle your index\n",
    "                perm = np.random.permutation(X_cross_train.shape[0])\n",
    "                        \n",
    "                X_cross_train = X_cross_train[perm]\n",
    "                y_cross_train = y_cross_train[perm]\n",
    "                \n",
    "                if   self.method == 'sto':\n",
    "                    for batch_idx in range(X_cross_train.shape[0]):\n",
    "                        X_method_train = X_cross_train[batch_idx].reshape(1, -1) #(11,) ==> (1, 11) ==> (m, n)\n",
    "                        y_method_train = y_cross_train[batch_idx]                    \n",
    "                        self._train(X_method_train, y_method_train)\n",
    "                elif self.method == 'mini':\n",
    "                    for batch_idx in range(0, X_cross_train.shape[0], self.batch_size):\n",
    "                        #batch_idx = 0, 50, 100, 150\n",
    "                        X_method_train = X_cross_train[batch_idx:batch_idx+self.batch_size, :]\n",
    "                        y_method_train = y_cross_train[batch_idx:batch_idx+self.batch_size]\n",
    "                        self._train(X_method_train, y_method_train)\n",
    "                else:\n",
    "                    X_method_train = X_cross_train\n",
    "                    y_method_train = y_cross_train\n",
    "                    self._train(X_method_train, y_method_train)\n",
    "                    \n",
    "                #Early Stopping\n",
    "                yhat_cross_train = self.predict(X_cross_train)\n",
    "                loss = mean_squared_error(y_cross_train, yhat_cross_train)\n",
    "                d_loss = abs(prev_loss - loss)\n",
    "                if d_loss > self.theshold:\n",
    "                    #ok, there is an improment\n",
    "                    prev_loss = loss\n",
    "                else:\n",
    "                    #done training. break\n",
    "                    break\n",
    "                \n",
    "            print(f\"Loss threshold achieved at num epoch: {epoch}\")\n",
    "\n",
    "            yhat_val = self.predict(X_cross_val)\n",
    "            self.kfold.append(mean_squared_error(y_cross_val, yhat_val))\n",
    "            print(f\"Fold {fold}: {mean_squared_error(y_cross_val, yhat_val)}\")\n",
    "                    \n",
    "    def _train(self, X, y):\n",
    "        yhat = self.predict(X)\n",
    "        grad = X.T @(yhat - y)\n",
    "        self.theta = self.theta - self.alpha * grad\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return X @ self.theta  #===>(m, n) @ (n, )\n",
    "    \n",
    "    def _coef(self):\n",
    "        return self.theta[1:]  #remind that theta is (w0, w1, w2, w3, w4.....wn)\n",
    "                               #w0 is the bias or the intercep\n",
    "                               #w1....wn are the weights / coefficients / theta\n",
    "        \n",
    "    def _bias(self):\n",
    "        return self.theta[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((442, 10), (442,))"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "diabetes = load_diabetes()\n",
    "\n",
    "X = diabetes.data\n",
    "y = diabetes.target\n",
    "\n",
    "#print the shape of X and y\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,\n",
    "                                                    random_state=999)\n",
    "\n",
    "# Assert X_train and y_train has same ammount of samples\n",
    "assert X_train.shape[0] == y_train.shape[0]\n",
    "\n",
    "# Assert X_test and y_test has same ammount of samples\n",
    "assert X_test.shape[0] == y_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "\n",
    "# Satndadize the trainning set\n",
    "X_train = sc.fit_transform(X_train)\n",
    "\n",
    "# Standardize the test set\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression_v2(method='batch',num_epochs=1000,theshold=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss threshold achieved at num epoch: 846\n",
      "Fold 0: 33323.04414751473\n",
      "Loss threshold achieved at num epoch: 268\n",
      "Fold 1: 28205.42602894653\n",
      "Loss threshold achieved at num epoch: 763\n",
      "Fold 2: 28455.087975256658\n",
      "Loss threshold achieved at num epoch: 740\n",
      "Fold 3: 29704.604613021125\n",
      "Loss threshold achieved at num epoch: 693\n",
      "Fold 4: 29511.55141799242\n"
     ]
    }
   ],
   "source": [
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion: \n",
    "- Even though `num_epochs` is set to 1000, in each fold the requierd loss threshold of 0.01 is achieved in less than 1000 iterations (846, 268, 736, 740, 693 iterations in the above example) after implementing 'Early Stopping'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23594.633554793396"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -7.8141485 ,  29.39367493,  16.61730099, -38.04383398,\n",
       "        40.3691605 ,   1.16936199, -13.25776384,  37.06672119,\n",
       "         3.39059881])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr._coef()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-7.2467994208371955"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr._bias()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Momentum\n",
    "\n",
    "`alpha` or `learning_rate` is the parameter how big of the step you will take on each Gradient optimization step.\n",
    "\n",
    "![overshoot](https://miro.medium.com/max/700/1*hGhRddOUV8h0pdQek8T35A.png)\n",
    "\n",
    "How much is `alpha` you should set?\n",
    "\n",
    "Not big such that it causes the *over shooting*.\n",
    "\n",
    "And certainly, not too small such that it causes you extra epochs to reach the minima.\n",
    "\n",
    "You want **just right** `alpha`.\n",
    "\n",
    "How do you know what is the right `alpha`?\n",
    "\n",
    "You guess, right?\n",
    "\n",
    "This time, 0.1. Observe the loss. If it is overshooting, stop and reduce the `alpha`. If the loss reduce slightly, maybe stop and increase the `alpha`.\n",
    "\n",
    "Finally, if you spend time adjusting the `alpha`, you will eventually find the **just right** `alpha`.\n",
    "\n",
    "Can we do better? Can we reduce the number of epochs without searching for the **just right** `alpha`.\n",
    "\n",
    "Yes, we can!!! Introducing!!!!!!!! MOMENTUM~~!!!!!!!!!\n",
    "\n",
    "### Momentum\n",
    "\n",
    "Here is the updating equation.\n",
    "\n",
    "$$ \\theta^{i+1} = \\theta^{i} - \\alpha \\times \\text{gradient}^i $$\n",
    "\n",
    "$i$ is the iteration/epoch number.\n",
    "\n",
    "$\\theta^{i+1}$ is the updated weights and will be used for the next iteration.\n",
    "\n",
    "$\\theta^{i}$ is the current weights of iteration $i$.\n",
    "\n",
    "$\\alpha$ is the learning rate.\n",
    "\n",
    "$\\text{gradient}^i$ is the step suggesting by the deviation of loss function from iteration $i$.\n",
    "\n",
    "$\\alpha \\times \\text{gradient}^i$ can be called *step*. When consider $i$, we call the $step^i$ how big of the step you take in iteration $i$.\n",
    "\n",
    "We know, in the early iteration, the step will be very big. Given the `alpha` is not too big, the step will decrase over time.\n",
    "\n",
    "Let's say our step size was big in the previous iteration (i-1), maybe, instead of taking $step^i$, we can take bigger step than $step^i$.\n",
    "\n",
    "Yes, and here is the equation.\n",
    "\n",
    "$$ \\theta^{i+1} = \\theta^{i} - step^i + \\text{momentum} \\times step^{i-1} $$\n",
    "\n",
    "Then, *momentum* here is how much do you want to include the information of previous step. The range of *momentum* is $[0.0 - 1.0]$. When *momentum* is 0 then you do not use previous step (basically, normal optimization). When it is 1 then you fully use the information from previous step. \n",
    "\n",
    "Now, code !!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "### pesudo code for people who has no idea\n",
    "\n",
    "# step = alpha * grad\n",
    "# theta = theta - step + momentum * prev_step\n",
    "# prev_step = step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "def mean_squared_error(ytrue, ypred):\n",
    "    return ((ypred - ytrue) ** 2).sum() / ytrue.shape[0]\n",
    "\n",
    "class LinearRegression_v3(object):\n",
    "    \n",
    "    kfold = KFold(n_splits=5)\n",
    "            \n",
    "    def __init__(self, alpha=0.001, num_epochs=5, batch_size=50, method='batch',\n",
    "                 cv=kfold, theshold=0.000001, momentum=0.01):\n",
    "        self.alpha      = alpha\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.method     = method\n",
    "        self.cv         = cv\n",
    "        self.theshold   = theshold\n",
    "        self.momentum   = momentum\n",
    "        self.prev_step = 0\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        \n",
    "        #using training......\n",
    "        \n",
    "        #please change it to cross-validation.....\n",
    "        \n",
    "        #create a list of kfold scores\n",
    "        self.kfold = list()\n",
    "\n",
    "        #Kfold.split in the sklearn.....\n",
    "        #5 splits\n",
    "        for fold, (train_idx, val_idx) in enumerate(self.cv.split(X_train)):\n",
    "            \n",
    "            X_cross_train = X_train[train_idx]\n",
    "            y_cross_train = y_train[train_idx]\n",
    "            X_cross_val   = X_train[val_idx]\n",
    "            y_cross_val   = y_train[val_idx]\n",
    "            \n",
    "            #create self.theta here\n",
    "            self.theta = np.zeros(X_cross_train.shape[1])\n",
    "            \n",
    "            #define X_cross_train as only a subset of the data\n",
    "            #how big is this subset?  => mini-batch size ==> 50\n",
    "\n",
    "            #Set previous loss to 0\n",
    "            prev_loss = 0\n",
    "            \n",
    "            #one epoch will exhaust the WHOLE training set\n",
    "            for epoch in range(self.num_epochs):\n",
    "            \n",
    "                #with replacement or no replacement\n",
    "                #with replacement means just randomize\n",
    "                #with no replacement means 0:50, 51:100, 101:150, ......300:323\n",
    "                #shuffle your index\n",
    "                #===> please shuffle your index\n",
    "                perm = np.random.permutation(X_cross_train.shape[0])\n",
    "                        \n",
    "                X_cross_train = X_cross_train[perm]\n",
    "                y_cross_train = y_cross_train[perm]\n",
    "                \n",
    "                if   self.method == 'sto':\n",
    "                    for batch_idx in range(X_cross_train.shape[0]):\n",
    "                        X_method_train = X_cross_train[batch_idx].reshape(1, -1) #(11,) ==> (1, 11) ==> (m, n)\n",
    "                        y_method_train = y_cross_train[batch_idx]                    \n",
    "                        self._train(X_method_train, y_method_train)\n",
    "                elif self.method == 'mini':\n",
    "                    for batch_idx in range(0, X_cross_train.shape[0], self.batch_size):\n",
    "                        #batch_idx = 0, 50, 100, 150\n",
    "                        X_method_train = X_cross_train[batch_idx:batch_idx+self.batch_size, :]\n",
    "                        y_method_train = y_cross_train[batch_idx:batch_idx+self.batch_size]\n",
    "                        self._train(X_method_train, y_method_train)\n",
    "                else:\n",
    "                    X_method_train = X_cross_train\n",
    "                    y_method_train = y_cross_train\n",
    "                    self._train(X_method_train, y_method_train)\n",
    "                    \n",
    "                #Early Stopping\n",
    "                yhat_cross_train = self.predict(X_cross_train)\n",
    "                loss = mean_squared_error(y_cross_train, yhat_cross_train)\n",
    "                d_loss = abs(prev_loss - loss)\n",
    "                if d_loss > self.theshold:\n",
    "                    #ok, there is an improment\n",
    "                    prev_loss = loss\n",
    "                else:\n",
    "                    #done training. break\n",
    "                    break\n",
    "                \n",
    "            print(f\"Loss threshold achieved at num epoch: {epoch}\")\n",
    "\n",
    "            yhat_val = self.predict(X_cross_val)\n",
    "            self.kfold.append(mean_squared_error(y_cross_val, yhat_val))\n",
    "            print(f\"Fold {fold}: {mean_squared_error(y_cross_val, yhat_val)}\")\n",
    "                    \n",
    "    def _train(self, X, y):\n",
    "        yhat = self.predict(X)\n",
    "        grad = X.T @(yhat - y)\n",
    "        step = self.alpha * grad\n",
    "        self.theta = self.theta - (step + self.momentum * self.prev_step)\n",
    "        self.prev_step = step\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return X @ self.theta  #===>(m, n) @ (n, )\n",
    "    \n",
    "    def _coef(self):\n",
    "        return self.theta[1:]  #remind that theta is (w0, w1, w2, w3, w4.....wn)\n",
    "                               #w0 is the bias or the intercep\n",
    "                               #w1....wn are the weights / coefficients / theta\n",
    "        \n",
    "    def _bias(self):\n",
    "        return self.theta[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression_v3(method='batch', num_epochs=1000, theshold=0.01, momentum=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss threshold achieved at num epoch: 833\n",
      "Fold 0: 33323.47826450363\n",
      "Loss threshold achieved at num epoch: 266\n",
      "Fold 1: 28205.068864447578\n",
      "Loss threshold achieved at num epoch: 753\n",
      "Fold 2: 28455.316725759985\n",
      "Loss threshold achieved at num epoch: 729\n",
      "Fold 3: 29704.682136980307\n",
      "Loss threshold achieved at num epoch: 684\n",
      "Fold 4: 29511.553635877586\n"
     ]
    }
   ],
   "source": [
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion: \n",
    "- After introducing 'Momentum' of 0.02, in each fold the number of iteratins have reduced further from (846, 268, 736, 740, 693) to (833, 266, 753, 729, 684) in the above example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23594.65594546635"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -7.81438346,  29.39343876,  16.61740747, -38.06128459,\n",
       "        40.38264417,   1.17721048, -13.25536685,  37.07334764,\n",
       "         3.39048634])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr._coef()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-7.24685663379659"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr._bias()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.venv': pipenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "27768773b483d82a9b2b839e3fa80b1be5789db7fd78df4eedef2df266871616"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
