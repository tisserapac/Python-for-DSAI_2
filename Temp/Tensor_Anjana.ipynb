{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where are we now\n",
    "1. Python - general problem solving\n",
    "2. Data Science - NumPy, Pandas, Sklearn, Matplotlib \n",
    "3. ML from Scratch - Intuition (so for those who want to further advance....)\n",
    "4. Signal Processing - Energy, Telecommunciations, Biosignals, Time Series\n",
    "5. Deep Learning - PyTorch\n",
    "   1. One of the most popular DL framework (against TensorFlow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning vs. Machine Learning\n",
    "\n",
    "Good News\n",
    "- Deep Learning can automatically feature engineer / feature selection\n",
    "- Deep Learning can benefit from huge amount of data, while Machine Learning cannot\n",
    "  - 100 samples vs 1000 samples, ML will get the same accuracy\n",
    "  - But DL will see increased accuracy\n",
    "- Deep Learning is basically stacking a lot of linear regression together\n",
    "  - DL can learn very complex patterns\n",
    "  - DL is perfect for (1) images, (2) text, (3) signal (very random)\n",
    "\n",
    "Bad News\n",
    "- Deep Learning sucks with small data (vs. Machine Learning) - 5000++ samples\n",
    "- For Tabular Data, Deep Learning will ALMOST ALWAYS LOSE TO gradient boosting (or its variants)\n",
    "  - Gradient Boosting is basically decision trees stacking after one another....\n",
    "  - For most competition, XGBoost and LightGBM are always the winners for tabular data\n",
    "  - If you work in a company, mostly they use tabular data, then you should look for gradient boosting types.... \n",
    "- Deep Learning has NO feature importance; so it's mostly blackbox....(Explanable AI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/projects/.venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch #pip install torch or pip3 install torch or conda install torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.23.3'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.12.1+cu102'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch Tensors\n",
    "\n",
    "PyTorch don't use NumPy.  Instead, it has its own data structures, called `Tensor`, which support automatic differentiation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create torch tensors from NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a numpy array of 1 to 5\n",
    "arr = np.arange(1, 6)\n",
    "# arr\n",
    "\n",
    "#print the data type\n",
    "arr.dtype  #int64\n",
    "\n",
    "#print the type()\n",
    "type(arr)  #belongs to Python itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert numpy to tensor\n",
    "\n",
    "#1. from_numpy (copy)\n",
    "torch_arr_from = torch.from_numpy(arr)\n",
    "torch_arr_from.dtype  #torch.int64\n",
    "type(torch_arr_from)  #torch.Tensor\n",
    "torch_arr_from.type() #torch.LongTensor (int64); if torch.IntTensor (int32)\n",
    "                      #torch.FloatTensor (float32); if torch.DoubleTensor (float64)\n",
    "#from_numpy is a copy!!!  This is intended, for easy use between numpy and tensor...\n",
    "# arr[2] = 999\n",
    "# torch_arr_from\n",
    "\n",
    "#2. tensor (not a copy)\n",
    "torch_arr_tensor = torch.tensor(arr)  #everything is the same, except it's NOT a copy\n",
    "arr[2] = 9999999\n",
    "torch_arr_tensor\n",
    "\n",
    "#In our class, mostly we use torch.tensor; it won't fail us :-)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some API to create tensor\n",
    "\n",
    "`torch.empty(size)` - any arbitrary numbers\n",
    "\n",
    "`torch.ones(size)`\n",
    "\n",
    "`torch.zeros(size)`\n",
    "\n",
    "`torch.arange(start, stop(ex), step)`\n",
    "\n",
    "`torch.linspace(start, stop, how many)`\n",
    "\n",
    "`torch.logspace(start, stop, how many)`  - power of 10\n",
    "\n",
    "`torch.rand(size)` - [0, 1)\n",
    "\n",
    "`torch.randn(size)` - std = 1 with uniform distribution\n",
    "\n",
    "`torch.randint(low, high, size)` - [low, high)\n",
    "\n",
    "`torch.ones_like(input)` = `torch.ones(input.shape)`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.4444, 1.8889, 2.3333, 2.7778, 3.2222, 3.6667, 4.1111, 4.5556,\n",
       "        5.0000])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr1 = torch.linspace(1, 5, 10)\n",
    "arr1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr2 = torch.rand((2,4))\n",
    "arr2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9797, 0.6278, 0.9673, 0.7281],\n",
       "        [0.6260, 0.1581, 0.6334, 0.0018]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0629,  0.3609, -0.3114, -0.1044,  0.4206]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.1236], requires_grad=True)\n",
      "torch.Size([1000, 1])\n"
     ]
    }
   ],
   "source": [
    "#import some deep learning layer\n",
    "#you have to help me create the right shape to insert to this layer\n",
    "\n",
    "import torch.nn as nn  #nn contains a lot of useful deep learning layers\n",
    "\n",
    "linear_layer = nn.Linear(5, 1)  #basically you insert 5 features, output 1 number\n",
    "print(linear_layer.weight)  #they treat this as theta, X @ theta^T\n",
    "print(linear_layer.bias)\n",
    "#[0.1315, 0.3990, 0.0960, 0.0807, 0.2908]\n",
    "#weight - [5, 1]\n",
    "#X @ weight\n",
    "#(anything, 5) @ (5, 1)\n",
    "\n",
    "#can you guys help me generate any pytorch tensor of size (?, ?)\n",
    "data   = torch.rand(1000, 5)\n",
    "output = linear_layer(data)\n",
    "print(output.shape)  #output shape?? - 1000, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(9999)  #this will make sure your weight is always init the same thing\n",
    "#this seed is VERY IMPORTANT for research\n",
    "#you CANNOT FORGET THIS - setting 5 different seeds is basically doing cross validation\n",
    "#please create two linear layers of size (100, 5), (5, 1)\n",
    "layer1 = nn.Linear(100, 5)\n",
    "layer2 = nn.Linear(5,   1)\n",
    "\n",
    "#try some input that pass through these two layers\n",
    "sample_size = 1000\n",
    "_input = torch.rand(sample_size, 100)\n",
    "# _input = layer1(_input)\n",
    "# _input = layer2(_input)\n",
    "# _input.shape\n",
    "\n",
    "#try nn.Sequential\n",
    "model = nn.Sequential(\n",
    "     layer1,\n",
    "     layer2\n",
    " )\n",
    "\n",
    "_input = model(_input)\n",
    "_input.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Changing the type\n",
    "#format .tyope()\n",
    "\n",
    "x = torch.arange(1,6)\n",
    "x.dtype\n",
    "\n",
    "x = x.type(torch.float64) # is NOT In Placae\n",
    "x.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshape and view\n",
    "- they are very similar\n",
    "- view will create a copy, while reshape does not\n",
    "- view will create a contiguous array, while reshape does not!\n",
    "- contiguous array - share consecutive memory x001 x002\n",
    "- non-contiguous array - memory in different places x001 x140 x004\n",
    "- some algorithms/models/cuda require your array to be contiguous\n",
    "  - in those case, use view or reshape to fix it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(10)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x.view(2,5)\n",
    "y[0, 0] = 9999\n",
    "y.is_contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9999,    1,    2,    3,    4,    5,    6,    7,    8,    9])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check x, does it change?\n",
    "x # X and y share memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = x.reshape(2,5) #Can or cannot be copy, see the documentation\n",
    "z.is_contiguous() #Not always contiguous, where as view is always contiguous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9999, 8888,    2,    3,    4,    5,    6,    7,    8,    9])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z[0,1] = 8888\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_transpose = z.transpose(1,0) #(5,2)\n",
    "z_transpose.shape\n",
    "z_transpose.is_contiguous()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradients\n",
    "\n",
    "### One step back-propagation\n",
    "\n",
    "$$y= 2x^4 + x^3 + 3X^2 + 5x + 1$$\n",
    "\n",
    "$$ dy /dx = y' = 8 x^3 + 3x^2 + 6x + 5$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "#What PyTorch is so amazing\n",
    "#It automatically calculate this gradient, always available\n",
    "\n",
    "x = torch.tensor(2.,requires_grad=True)\n",
    "print(x.grad) # No derivative calculated yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(63., grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = 2*x**4 + x**3 + 3*x**2 + 5*x + 1\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward() # is in palce function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(93.)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#try now print x.grad # this will be basically dy/dx at point x = 2\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ dy /dx = y' = 8 x^3 + 3x^2 + 6x + 5$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check how we get 93\n",
    "y_der = 8*2**3 + 3*2**2 + 6*2 + 5\n",
    "y_der \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple step back-propagation\n",
    "\n",
    "$$y = 3x + 2$$\n",
    "$$z = 2y^2  $$\n",
    "$$o = z / 6 $$ \n",
    "\n",
    "- let's assume we have 6 elements\n",
    "\n",
    "$$\\frac{\\partial o}{\\partial x} = \\frac{\\partial o}{\\partial z} * \\frac{\\partial z}{\\partial y} * \\frac{\\partial y}{\\partial x}$$\n",
    "\n",
    "$$\\frac{\\partial o}{\\partial z} = 1/6$$\n",
    "\n",
    "$$\\frac{\\partial z}{\\partial y} = 4(y) = 4(3x + 2) $$\n",
    "\n",
    "$$\\frac{\\partial y}{\\partial x} = 3$$\n",
    "\n",
    "$$\\frac{\\partial o}{\\partial x} =2(3x + 2)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1.,2.,3.],[3.,2.,1.]], requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = 3*x + 2\n",
    "z = 2*y**2\n",
    "o = z.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "o.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10., 16., 22.],\n",
       "        [22., 16., 10.]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad\n",
    "# How did we get 10, 16, 22?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_grad = 2 * (3*1 + 2)\n",
    "x_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "### Use the same x\n",
    "\n",
    "y = 10x - 9999\n",
    "z = 5 - y \n",
    "o = 3z**2\n",
    "oo = o /6\n",
    "\n",
    "dy/dx = 10\n",
    "dz/dy = -1\n",
    "do/dz = 6*z\n",
    "doo / do = 1/6 \n",
    "do0/dx = -10(z) = -10(5-y) = -10(5-10x + 9999) = 100x - 100040\n",
    "\n",
    "\n",
    "### Task1: calculalate all the gradient\n",
    "### Task2: Code and try whether it matches with your answer\n",
    "### Put on the chat if you are done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-99940., -99840., -99740.],\n",
      "        [-99740., -99840., -99940.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1., 2., 3.],[3.,2.,1.]],requires_grad=True)\n",
    "y = 10*x-9999\n",
    "z = 5 - y\n",
    "o = 3*z**2\n",
    "\n",
    "o = o.mean() # We have to make it into a number .... for back propagation\n",
    "o.backward()\n",
    "print(x.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-99940"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100 - 100040"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "x = torch.tensor([[1., 2., 3.],[3.,2.,1.]],requires_grad=True)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X = [\n",
    "    x11 x12 x13\n",
    "    x21 x22 x23\n",
    "]\n",
    "\n",
    "W = [\n",
    "    W1\n",
    "    W2\n",
    "    W3\n",
    "    ]\n",
    "\n",
    "x @ w = [\n",
    "    x11*W1 x12*W2 x13*W3\n",
    "    x21*W1 x22*W2 x23*W3\n",
    "                            ]\n",
    "\n",
    "dW/dX = [\n",
    "            \n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [1.],\n",
       "        [2.]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.arange(3.).view(3,1)\n",
    "w.shape # (3,1)\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = x @ w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "oo = o.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "oo.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6., grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 2.],\n",
      "        [0., 1., 2.]])\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m torch\u001b[39m.\u001b[39;49mcuda\u001b[39m.\u001b[39;49mcurrent_device()\n",
      "File \u001b[0;32m~/projects/.venv/lib/python3.8/site-packages/torch/cuda/__init__.py:482\u001b[0m, in \u001b[0;36mcurrent_device\u001b[0;34m()\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcurrent_device\u001b[39m() \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mint\u001b[39m:\n\u001b[1;32m    481\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Returns the index of a currently selected device.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 482\u001b[0m     _lazy_init()\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_cuda_getDevice()\n",
      "File \u001b[0;32m~/projects/.venv/lib/python3.8/site-packages/torch/cuda/__init__.py:217\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[1;32m    214\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    215\u001b[0m \u001b[39m# This function throws if there's a driver initialization error, no GPUs\u001b[39;00m\n\u001b[1;32m    216\u001b[0m \u001b[39m# are found or any other error occurs\u001b[39;00m\n\u001b[0;32m--> 217\u001b[0m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_cuda_init()\n\u001b[1;32m    218\u001b[0m \u001b[39m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[39m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[39m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    221\u001b[0m _tls\u001b[39m.\u001b[39mis_initializing \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
     ]
    }
   ],
   "source": [
    "#torch.cuda.current_device()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.venv': pipenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "27768773b483d82a9b2b839e3fa80b1be5789db7fd78df4eedef2df266871616"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
