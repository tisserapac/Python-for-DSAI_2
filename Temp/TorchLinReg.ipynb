{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")\n",
    "print(\"Device: \", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ETL\n",
    "    1. Specifying some random input\n",
    "    2. Pytorch dataset and dataloader\n",
    "2. EDA ... will not be done today\n",
    "3. Feature Engineering/ cleaning ... no need to do\n",
    "4. Modeling\n",
    "    3. nn.linear\n",
    "    4. Define loss fn.\n",
    "    5. Define the optimizer fn.\n",
    "    6. Train the model\n",
    "5. Inference/ Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider this data:\n",
    "\n",
    "<img src = \"figures/japan.png\" width=\"400\">\n",
    "\n",
    "In a linear regression model, each target variable is estimated to be a weighted sum of the input variables, offset by some constant, known as a bias :\n",
    "\n",
    "$$\\text{yield}_\\text{apple}  = w_{11} * \\text{temp} + w_{12} * \\text{rainfall} + w_{13} * \\text{humidity} + b_{1}$$\n",
    "\n",
    "$$\\text{yield}_\\text{orange} = w_{21} * \\text{temp} + w_{22} * \\text{rainfall} + w_{23} * \\text{humidity} + b_{2}$$\n",
    "\n",
    "Visually, it means that the yield of apples is a linear or planar function of temperature, rainfall and humidity:\n",
    "\n",
    "<img src = \"figures/japan2.png\" width=\"400\">\n",
    "\n",
    "The learning part of linear regression is to figure out a set of weights <code>w11, w12,... w23, b1 \\& b2</code> using gradient descent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X(temp, rainfall, hum)\n",
    "\n",
    "X_train = np.array([[73, 67, 43], [91, 88, 64], [87, 134, 58], \n",
    "                   [102, 43, 37], [69, 96, 70], [73, 67, 43], \n",
    "                   [91, 88, 64], [87, 134, 58], [102, 43, 37], \n",
    "                   [69, 96, 70], [73, 67, 43], [91, 88, 64], \n",
    "                   [87, 134, 58], [102, 43, 37], [69, 96, 70]], \n",
    "                  dtype='float32')\n",
    "\n",
    "# Targets (apples, oranges)\n",
    "Y_train = np.array([[56, 70], [81, 101], [119, 133], \n",
    "                    [22, 37], [103, 119], [56, 70], \n",
    "                    [81, 101], [119, 133], [22, 37], \n",
    "                    [103, 119], [56, 70], [81, 101], \n",
    "                    [119, 133], [22, 37], [103, 119]], \n",
    "                   dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15, 3])\n",
      "torch.Size([15, 2])\n"
     ]
    }
   ],
   "source": [
    "#Create tensors from the np. arr.\n",
    "\n",
    "inputs = torch.tensor(X_train)\n",
    "targets = torch.tensor(Y_train)\n",
    "\n",
    "#Print the shape\n",
    "print(inputs.shape)\n",
    "print(targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Dataset\n",
    "\n",
    "- We are going to crate a TensorDataset on top of these tensors, so we can access each row from the input and target tuples.\n",
    "\n",
    "- Note - This is neded if we want to use the DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X.shape (m,n) Y.sahpe (m,k)\n",
    "ds = TensorDataset(inputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([91., 88., 64.]), tensor([ 81., 101.]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the format PyTorch wants\n",
    "# A tuple of two tensors, the x and coresponding  y\n",
    "ds[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([91., 88., 64.])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 DataLoader\n",
    "\n",
    "- By default PyTorch works in batch.\n",
    "\n",
    "- In simple words it will ALWAYS take some mini-batch and performs gradient descent\n",
    "\n",
    "- Mini-batch - beacuse assume you won't be able be able to fit in 1 M samples into GPU RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will automatically createsa an enumarator , look at each batch\n",
    "# Means you can simply perform a for loop onto DataLoader\n",
    "# If DataLoader is not used we have to manually selectthe mini-batch\n",
    "# Randomized\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 3 # Can be any number\n",
    "# Too small - slow\n",
    "# Too large - run out of memory\n",
    "dl = DataLoader(ds, batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[69., 96., 70.],\n",
      "        [73., 67., 43.],\n",
      "        [73., 67., 43.]]), tensor([[103., 119.],\n",
      "        [ 56.,  70.],\n",
      "        [ 56.,  70.]])]\n"
     ]
    }
   ],
   "source": [
    "# Now this dl is basically an enumerator, in which we can loop on....\n",
    "for something in dl:\n",
    "    print(something)   # Three X+Y tuples\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X : tensor([[ 73.,  67.,  43.],\n",
      "        [ 87., 134.,  58.],\n",
      "        [ 73.,  67.,  43.]])\n",
      "Y : tensor([[ 56.,  70.],\n",
      "        [119., 133.],\n",
      "        [ 56.,  70.]])\n"
     ]
    }
   ],
   "source": [
    "for x, y in dl:\n",
    "    print(f\"X : {x}\")\n",
    "    print(f\"Y : {y}\")\n",
    "    break\n",
    "\n",
    "# The dl keeps an internal counter\n",
    "# This dl is keep on running : which is intentional, we have the concept of \"epochs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Define our neural network\n",
    "\n",
    "- How many layers we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "#Define our NN here\n",
    "#Just one layer at the moment\n",
    "# Later add one more layer\n",
    "# Format nn.Linear(in_features, out_features)\n",
    "# Format nn.Linear(temp;rainfalll;hum, oranges;apples)\n",
    "model = nn.Linear(3,2) # <-- Hidden later\n",
    " \n",
    "# Linerar layers are simple matrix multiplication...\n",
    "# Many other names, In Keras we called Dense, In TensorFlow we called FullyConnected\n",
    "# Keras very high level, not good for research, devlopment \n",
    "# TensorFlow is developed by Google, it is quite good, \n",
    "\n",
    "# Fore very huge complex high performance models TensorFlow is much better, optimized\n",
    "# Very low-level than Pytorch\n",
    "#For very general almost any model that we use in even un reserach - PyTorch is much beteer\n",
    "# Due to its computational graph\n",
    "\n",
    "# TensorFlow something called TensorFlowLite which is the way.\n",
    "# You want to use for mobile phones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I wonder whether having one extra layer will reduce the loss.\n",
    "\n",
    "# model = nn.Sequential(\n",
    "#     nn.Linear()\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class is the best practice for creating a neural network\n",
    "\n",
    "#format\n",
    "'''\n",
    "class AnyNameCapitalized(nn.Module): # It is basically inherin nn.Module\n",
    "    def __init()__:\n",
    "        super(.__init()__ #super is pasically inheriting nn.Module init\n",
    "        #We define the layers here\n",
    "    \n",
    "'''\n",
    "\n",
    "# class NeuralNetwork(nn.Module):\n",
    "    \n",
    "#     def __init__(self, input_size, hidden_size, output_size):\n",
    "#         super.__init__()\n",
    "#         self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "#         self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         out = self.fc1(x)\n",
    "#         out = self.fc2(out)\n",
    "#         return out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = NeuralNetwork (3, 99, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.weight # By default these weights are uniformly close to 0\n",
    "model.fc1.weight\n",
    "model.fc2.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weight.shape #Tis one is basically in the shape (out_feature, in_feature)\n",
    "\n",
    "# You can imagine X @ W^T\n",
    "# After you transpose W, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.0958,  0.4890], requires_grad=True)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.bias\n",
    "\n",
    "# Why two bias, beacause y1 bias and y2 bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.2432,  0.4726,  0.4454],\n",
       "         [ 0.4577,  0.5533, -0.2382]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.0958,  0.4890], requires_grad=True)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#p.numel() just flattern everythin\n",
    "sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Why 8 - 6 weights and 2 bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15, 3])\n",
      "tensor([[ 68.4779,  60.7320],\n",
      "        [ 92.1343,  75.5885],\n",
      "        [110.2310, 100.6378],\n",
      "        [ 61.5157,  62.1564],\n",
      "        [ 93.2368,  68.5156],\n",
      "        [ 68.4779,  60.7320],\n",
      "        [ 92.1343,  75.5885],\n",
      "        [110.2310, 100.6378],\n",
      "        [ 61.5157,  62.1564],\n",
      "        [ 93.2368,  68.5156],\n",
      "        [ 68.4779,  60.7320],\n",
      "        [ 92.1343,  75.5885],\n",
      "        [110.2310, 100.6378],\n",
      "        [ 61.5157,  62.1564],\n",
      "        [ 93.2368,  68.5156]], grad_fn=<AddmmBackward0>)\n",
      "torch.Size([15, 2])\n"
     ]
    }
   ],
   "source": [
    "print(inputs.shape)\n",
    "\n",
    "output = model(inputs) # (15,3) @ (3, 2) = (15, 2)\n",
    "\n",
    "print(output)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Define the loss fn.\n",
    "\n",
    "- Should ber MSE or Cross Entrophy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Under the nn mudule , there are many loss function\n",
    "\n",
    "J_fn = nn.MSELoss()\n",
    "\n",
    "#Later on you will know how to use this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Define the optimizer\n",
    "1. Predict\n",
    "2. Loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normally in skleran we call fit, it will perform gradient descent\n",
    "# In code from scratch we need to like specify how we want to update the gradients\n",
    "# Optimizer handles how we update the parameters\n",
    "# If we use w = w -alpha (gradient) ==> gradient descent\n",
    "#Stochastic gradient descent ==>  is not one sample - mini-batch\n",
    "\n",
    "optim = torch.optim.SGD(model.parameters(), lr=0.0001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Actually tran the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5 # It depends ... trian and error...\n",
    "def fit():\n",
    "    #For num_epochs\n",
    "        #for dataloader\n",
    "         #1. predict(forward pass )\n",
    "         #2. calculate loss\n",
    "         #3. calculate gradient\n",
    "         #4. update the parameters using the optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 - Loss: 756.319091796875\n",
      "Epoch: 0 - Loss: 2455.9990234375\n",
      "Epoch: 0 - Loss: 2432.283935546875\n",
      "Epoch: 0 - Loss: 790.4541015625\n",
      "Epoch: 0 - Loss: 121.9244613647461\n",
      "Epoch: 1 - Loss: 126.046630859375\n",
      "Epoch: 1 - Loss: 436.5860595703125\n",
      "Epoch: 1 - Loss: 837.3534545898438\n",
      "Epoch: 1 - Loss: 462.3238525390625\n",
      "Epoch: 1 - Loss: 742.0138549804688\n",
      "Epoch: 2 - Loss: 306.7709045410156\n",
      "Epoch: 2 - Loss: 118.33515167236328\n",
      "Epoch: 2 - Loss: 470.9854431152344\n",
      "Epoch: 2 - Loss: 730.1071166992188\n",
      "Epoch: 2 - Loss: 730.603759765625\n",
      "Epoch: 3 - Loss: 633.5977172851562\n",
      "Epoch: 3 - Loss: 618.3212890625\n",
      "Epoch: 3 - Loss: 296.24188232421875\n",
      "Epoch: 3 - Loss: 222.3563690185547\n",
      "Epoch: 3 - Loss: 106.81807708740234\n",
      "Epoch: 4 - Loss: 142.64190673828125\n",
      "Epoch: 4 - Loss: 115.96732330322266\n",
      "Epoch: 4 - Loss: 47.31418228149414\n",
      "Epoch: 4 - Loss: 93.90081787109375\n",
      "Epoch: 4 - Loss: 176.44415283203125\n",
      "Epoch: 5 - Loss: 41.66023635864258\n",
      "Epoch: 5 - Loss: 31.12462043762207\n",
      "Epoch: 5 - Loss: 89.86382293701172\n",
      "Epoch: 5 - Loss: 126.4339828491211\n",
      "Epoch: 5 - Loss: 86.48152923583984\n",
      "Epoch: 6 - Loss: 181.0302276611328\n",
      "Epoch: 6 - Loss: 390.9682312011719\n",
      "Epoch: 6 - Loss: 228.3900146484375\n",
      "Epoch: 6 - Loss: 47.0941276550293\n",
      "Epoch: 6 - Loss: 83.52587127685547\n",
      "Epoch: 7 - Loss: 140.87513732910156\n",
      "Epoch: 7 - Loss: 51.441619873046875\n",
      "Epoch: 7 - Loss: 32.646209716796875\n",
      "Epoch: 7 - Loss: 112.55899810791016\n",
      "Epoch: 7 - Loss: 213.5543670654297\n",
      "Epoch: 8 - Loss: 187.8670654296875\n",
      "Epoch: 8 - Loss: 67.12171936035156\n",
      "Epoch: 8 - Loss: 38.6372184753418\n",
      "Epoch: 8 - Loss: 40.52426528930664\n",
      "Epoch: 8 - Loss: 35.866485595703125\n",
      "Epoch: 9 - Loss: 132.66944885253906\n",
      "Epoch: 9 - Loss: 72.43291473388672\n",
      "Epoch: 9 - Loss: 33.5400505065918\n",
      "Epoch: 9 - Loss: 32.90829086303711\n",
      "Epoch: 9 - Loss: 6.909786224365234\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10 # It depends ... trian and error...\n",
    "for epoch in range(num_epochs):\n",
    "    #for dataloader\n",
    "    for x, y in dl: #What is the shape of x and y here\n",
    "        #x and y are the size of the mini-batch of X_train and y_train\n",
    "        #batch size is 3\n",
    "        # x (3,3) y (3,2)\n",
    "        # X: (batch, feature) = (3,3)\n",
    "        # y: (batch, target) = (3,2)\n",
    "        x.to(device)# Device is either cpu or cuda\n",
    "        y.to(device)\n",
    "\n",
    "        #1. Predict (forward pass)\n",
    "        yhat = model(x)\n",
    "\n",
    "        #2. Calculate loss\n",
    "        #format J_fn(inputs, targets)\n",
    "        loss = J_fn(yhat, y)\n",
    "\n",
    "        #3. Calculate gradient\n",
    "        #3.1 clear out the previous gradients\n",
    "        optim.zero_grad()\n",
    "        #3.2 Call backwars() on loss to retrive all the gradients (backpropagation)\n",
    "        loss.backward()\n",
    "        #backward DOES not adjust the weight Yet, just backpropagation\n",
    "        #We want to calculate the gradients of all the parameters (8-6 weights and 2 bias)\n",
    "        #IN RESPECT TO the LOSS... dJ/dw11, dJ/dw2, dJ/dw13..., dJ/db1, dJ/db2\n",
    "\n",
    "        #4. Update the parameters using the optim\n",
    "        # W = W - alpha * gradient - no need to do this here\n",
    "        optim.step() # optim already kow the learning rate and has the parameters\n",
    "\n",
    "        print(f\"Epoch: {epoch} - Loss: {loss}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inference / Testing\n",
    "\n",
    "Test some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([73., 67., 43.]), tensor([56., 70.]))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a np array of \n",
    "# [74, 68, 42] , [92, 88, 65]\n",
    "\n",
    "X_test_np = np.array([[74., 68., 42.],[92., 88., 65.]], dtype='float32')\n",
    "\n",
    "\n",
    "# Please make a tensor\n",
    "X_test = torch.tensor(X_test_np)\n",
    "\n",
    "# Then use our model to predic the number of orenges and apples\n",
    "yhat = model(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[58.2936, 70.0953],\n",
       "        [83.0424, 94.3229]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 56.,  70.],\n",
       "        [ 81., 101.]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the loss comparing ds[0] and ds[1]\n",
    "ytest = ds[0:2][1]\n",
    "ytest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(13.5063, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = J_fn(yhat, ytest)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.venv': pipenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "27768773b483d82a9b2b839e3fa80b1be5789db7fd78df4eedef2df266871616"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
