{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")\n",
    "print(\"Device: \", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plan for today:\n",
    "\n",
    "1. ETL \n",
    "   1. Specifying some some random input\n",
    "   2. PyTorch Dataset and DataLoader\n",
    "2. EDA - we gonna just skip because we are lazy...\n",
    "3. Feature Engineering / Cleaning - which we don't need to....\n",
    "4. Modeling \n",
    "   1. `nn.Linear` (luckily, you already understand this!  Yay!)\n",
    "   2. Define loss function (mse for regression, cross entrophy for classification)\n",
    "   3. Define the optimizer function (gradient descent ; adam)\n",
    "   4. Train the model\n",
    "5. Inference / Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider this data:\n",
    "\n",
    "<img src = \"figures/japan.png\" width=\"400\">\n",
    "\n",
    "In a linear regression model, each target variable is estimated to be a weighted sum of the input variables, offset by some constant, known as a bias :\n",
    "\n",
    "$$\\text{yield}_\\text{apple}  = w_{11} * \\text{temp} + w_{12} * \\text{rainfall} + w_{13} * \\text{humidity} + b_{1}$$\n",
    "\n",
    "$$\\text{yield}_\\text{orange} = w_{21} * \\text{temp} + w_{22} * \\text{rainfall} + w_{23} * \\text{humidity} + b_{2}$$\n",
    "\n",
    "Visually, it means that the yield of apples is a linear or planar function of temperature, rainfall and humidity:\n",
    "\n",
    "<img src = \"figures/japan2.png\" width=\"400\">\n",
    "\n",
    "The learning part of linear regression is to figure out a set of weights <code>w11, w12,... w23, b1 \\& b2</code> using gradient descent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X(temp, rainfall, hum)\n",
    "\n",
    "X_train = np.array([[73, 67, 43], [91, 88, 64], [87, 134, 58], \n",
    "                   [102, 43, 37], [69, 96, 70], [73, 67, 43], \n",
    "                   [91, 88, 64], [87, 134, 58], [102, 43, 37], \n",
    "                   [69, 96, 70], [73, 67, 43], [91, 88, 64], \n",
    "                   [87, 134, 58], [102, 43, 37], [69, 96, 70]], \n",
    "                  dtype='float32')\n",
    "\n",
    "# Targets (apples, oranges)\n",
    "Y_train = np.array([[56, 70], [81, 101], [119, 133], \n",
    "                    [22, 37], [103, 119], [56, 70], \n",
    "                    [81, 101], [119, 133], [22, 37], \n",
    "                    [103, 119], [56, 70], [81, 101], \n",
    "                    [119, 133], [22, 37], [103, 119]], \n",
    "                   dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15, 3])\n",
      "torch.Size([15, 2])\n"
     ]
    }
   ],
   "source": [
    "#Create tensors from the np. arr.\n",
    "\n",
    "inputs = torch.tensor(X_train)\n",
    "targets = torch.tensor(Y_train)\n",
    "\n",
    "#Print the shape\n",
    "print(inputs.shape)\n",
    "print(targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Dataset\n",
    "\n",
    "- We are going to crate a TensorDataset on top of these tensors, so we can access each row from the input and target tuples.\n",
    "\n",
    "- Note - This is neded if we want to use the DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X.shape (m,n) Y.sahpe (m,k)\n",
    "ds = TensorDataset(inputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([91., 88., 64.]), tensor([ 81., 101.]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the format PyTorch wants\n",
    "# A tuple of two tensors, the x and coresponding  y\n",
    "ds[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([91., 88., 64.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 DataLoader\n",
    "\n",
    "- By default PyTorch works in batch.\n",
    "\n",
    "- In simple words it will ALWAYS take some mini-batch and performs gradient descent\n",
    "\n",
    "- Mini-batch - beacuse assume you won't be able be able to fit in 1 M samples into GPU RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will automatically createsa an enumarator , look at each batch\n",
    "# Means you can simply perform a for loop onto DataLoader\n",
    "# If DataLoader is not used we have to manually selectthe mini-batch\n",
    "# Randomized\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 3 # Can be any number\n",
    "# Too small - slow\n",
    "# Too large - run out of memory\n",
    "dl = DataLoader(ds, batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 69.,  96.,  70.],\n",
      "        [102.,  43.,  37.],\n",
      "        [ 73.,  67.,  43.]]), tensor([[103., 119.],\n",
      "        [ 22.,  37.],\n",
      "        [ 56.,  70.]])]\n"
     ]
    }
   ],
   "source": [
    "# Now this dl is basically an enumerator, in which we can loop on....\n",
    "for something in dl:\n",
    "    print(something)   # Three X+Y tuples\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X : tensor([[102.,  43.,  37.],\n",
      "        [ 91.,  88.,  64.],\n",
      "        [ 87., 134.,  58.]])\n",
      "Y : tensor([[ 22.,  37.],\n",
      "        [ 81., 101.],\n",
      "        [119., 133.]])\n"
     ]
    }
   ],
   "source": [
    "for x, y in dl:\n",
    "    print(f\"X : {x}\")\n",
    "    print(f\"Y : {y}\")\n",
    "    break\n",
    "\n",
    "# The dl keeps an internal counter\n",
    "# This dl is keep on running : which is intentional, we have the concept of \"epochs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Define our neural network\n",
    "\n",
    "- How many layers we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "#Define our NN here\n",
    "#Just one layer at the moment\n",
    "# Later add one more layer\n",
    "# Format nn.Linear(in_features, out_features)\n",
    "# Format nn.Linear(temp;rainfalll;hum, oranges;apples)\n",
    "model = nn.Linear(3,2) # <-- Hidden later\n",
    " \n",
    "# Linerar layers are simple matrix multiplication...\n",
    "# Many other names, In Keras we called Dense, In TensorFlow we called FullyConnected\n",
    "# Keras very high level, not good for research, devlopment \n",
    "# TensorFlow is developed by Google, it is quite good, \n",
    "\n",
    "# Fore very huge complex high performance models TensorFlow is much better, optimized\n",
    "# Very low-level than Pytorch\n",
    "#For very general almost any model that we use in even un reserach - PyTorch is much beteer\n",
    "# Due to its computational graph\n",
    "\n",
    "# TensorFlow something called TensorFlowLite which is the way.\n",
    "# You want to use for mobile phones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I wonder whether having one extra layer will reduce the loss.\n",
    "\n",
    "# model = nn.Sequential(\n",
    "#     nn.Linear()\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class is the best practice for creating a neural network\n",
    "\n",
    "#format\n",
    "'''\n",
    "class AnyNameCapitalized(nn.Module): # It is basically inherin nn.Module\n",
    "    def __init()__:\n",
    "        super(.__init()__ #super is pasically inheriting nn.Module init\n",
    "        #We define the layers here\n",
    "    \n",
    "'''\n",
    "\n",
    "# class NeuralNetwork(nn.Module):\n",
    "    \n",
    "#     def __init__(self, input_size, hidden_size, output_size):\n",
    "#         super.__init__()\n",
    "#         self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "#         self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         out = self.fc1(x)\n",
    "#         out = self.fc2(out)\n",
    "#         return out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = NeuralNetwork (3, 99, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.5724,  0.1971, -0.0043],\n",
       "        [-0.3552,  0.4748, -0.0542]], requires_grad=True)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weight # By default these weights are uniformly close to 0\n",
    "# model.fc1.weight\n",
    "# model.fc2.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weight.shape #This one is basically in the shape (out_feature, in_feature)\n",
    "\n",
    "# You can imagine X @ W^T\n",
    "# After you transpose W, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.5489, 0.0992], requires_grad=True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.bias\n",
    "\n",
    "# Why two bias, beacause y1 bias and y2 bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.5724,  0.1971, -0.0043],\n",
       "         [-0.3552,  0.4748, -0.0542]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.5489, 0.0992], requires_grad=True)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#p.numel() just flattern everythin\n",
    "sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Why 8 - 6 weights and 2 bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15, 3])\n",
      "tensor([[-28.2163,   3.6504],\n",
      "        [-34.4712,   6.0897],\n",
      "        [-23.0889,  29.6762],\n",
      "        [-49.5201, -17.7209],\n",
      "        [-20.3278,  17.3778],\n",
      "        [-28.2163,   3.6504],\n",
      "        [-34.4712,   6.0897],\n",
      "        [-23.0889,  29.6762],\n",
      "        [-49.5201, -17.7209],\n",
      "        [-20.3278,  17.3778],\n",
      "        [-28.2163,   3.6504],\n",
      "        [-34.4712,   6.0897],\n",
      "        [-23.0889,  29.6762],\n",
      "        [-49.5201, -17.7209],\n",
      "        [-20.3278,  17.3778]], grad_fn=<AddmmBackward0>)\n",
      "torch.Size([15, 2])\n"
     ]
    }
   ],
   "source": [
    "print(inputs.shape)\n",
    "\n",
    "output = model(inputs) # (15,3) @ (3, 2) = (15, 2)\n",
    "\n",
    "print(output)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Define the loss fn.\n",
    "\n",
    "- Should ber MSE or Cross Entrophy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Under the nn mudule , there are many loss function\n",
    "\n",
    "J_fn = nn.MSELoss()\n",
    "\n",
    "#Later on you will know how to use this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Define the optimizer\n",
    "1. Predict\n",
    "2. Loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normally in skleran we call fit, it will perform gradient descent\n",
    "# In code from scratch we need to like specify how we want to update the gradients\n",
    "# Optimizer handles how we update the parameters\n",
    "# If we use w = w -alpha (gradient) ==> gradient descent\n",
    "#Stochastic gradient descent ==>  is not one sample - mini-batch\n",
    "\n",
    "optim = torch.optim.SGD(model.parameters(), lr=0.0001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Actually train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5 # It depends ... trian and error...\n",
    "def fit():\n",
    "    #For num_epochs\n",
    "        #for dataloader\n",
    "         #1. predict(forward pass )\n",
    "         #2. calculate loss\n",
    "         #3. calculate gradient\n",
    "         #4. update the parameters using the optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 - Loss: 9863.8564453125\n",
      "Epoch: 0 - Loss: 5102.54052734375\n",
      "Epoch: 0 - Loss: 4382.27587890625\n",
      "Epoch: 0 - Loss: 3251.746337890625\n",
      "Epoch: 0 - Loss: 860.6041870117188\n",
      "Epoch: 1 - Loss: 331.7251281738281\n",
      "Epoch: 1 - Loss: 78.15120697021484\n",
      "Epoch: 1 - Loss: 115.98177337646484\n",
      "Epoch: 1 - Loss: 58.624481201171875\n",
      "Epoch: 1 - Loss: 122.74605560302734\n",
      "Epoch: 2 - Loss: 45.06119918823242\n",
      "Epoch: 2 - Loss: 58.6786003112793\n",
      "Epoch: 2 - Loss: 35.11232376098633\n",
      "Epoch: 2 - Loss: 56.438167572021484\n",
      "Epoch: 2 - Loss: 26.329345703125\n",
      "Epoch: 3 - Loss: 68.3761978149414\n",
      "Epoch: 3 - Loss: 69.85672760009766\n",
      "Epoch: 3 - Loss: 60.91935348510742\n",
      "Epoch: 3 - Loss: 68.0940933227539\n",
      "Epoch: 3 - Loss: 43.65385055541992\n",
      "Epoch: 4 - Loss: 91.66065216064453\n",
      "Epoch: 4 - Loss: 114.28926849365234\n",
      "Epoch: 4 - Loss: 56.47148513793945\n",
      "Epoch: 4 - Loss: 72.1084213256836\n",
      "Epoch: 4 - Loss: 117.1617660522461\n",
      "Epoch: 5 - Loss: 141.39515686035156\n",
      "Epoch: 5 - Loss: 22.76177978515625\n",
      "Epoch: 5 - Loss: 34.39036178588867\n",
      "Epoch: 5 - Loss: 37.76472091674805\n",
      "Epoch: 5 - Loss: 106.6375503540039\n",
      "Epoch: 6 - Loss: 67.69605255126953\n",
      "Epoch: 6 - Loss: 2.3103444576263428\n",
      "Epoch: 6 - Loss: 12.199748039245605\n",
      "Epoch: 6 - Loss: 25.790102005004883\n",
      "Epoch: 6 - Loss: 25.85401725769043\n",
      "Epoch: 7 - Loss: 20.99553108215332\n",
      "Epoch: 7 - Loss: 78.85643768310547\n",
      "Epoch: 7 - Loss: 92.5887680053711\n",
      "Epoch: 7 - Loss: 24.734540939331055\n",
      "Epoch: 7 - Loss: 57.77750015258789\n",
      "Epoch: 8 - Loss: 184.44744873046875\n",
      "Epoch: 8 - Loss: 295.3433532714844\n",
      "Epoch: 8 - Loss: 367.41455078125\n",
      "Epoch: 8 - Loss: 309.3749694824219\n",
      "Epoch: 8 - Loss: 48.36306381225586\n",
      "Epoch: 9 - Loss: 37.89925765991211\n",
      "Epoch: 9 - Loss: 15.695435523986816\n",
      "Epoch: 9 - Loss: 12.03430461883545\n",
      "Epoch: 9 - Loss: 35.61246871948242\n",
      "Epoch: 9 - Loss: 42.982425689697266\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10 # It depends ... trian and error...\n",
    "for epoch in range(num_epochs):\n",
    "    #for dataloader\n",
    "    for x, y in dl: #What is the shape of x and y here\n",
    "        #x and y are the size of the mini-batch of X_train and y_train\n",
    "        #batch size is 3\n",
    "        # x (3,3) y (3,2)\n",
    "        # X: (batch, feature) = (3,3)\n",
    "        # y: (batch, target) = (3,2)\n",
    "        x.to(device)# Device is either cpu or cuda\n",
    "        y.to(device)\n",
    "\n",
    "        #1. Predict (forward pass)\n",
    "        yhat = model(x)\n",
    "\n",
    "        #2. Calculate loss\n",
    "        #format J_fn(inputs, targets)\n",
    "        loss = J_fn(yhat, y)\n",
    "\n",
    "        #3. Calculate gradient\n",
    "        #3.1 clear out the previous gradients\n",
    "        optim.zero_grad()\n",
    "        #3.2 Call backwars() on loss to retrive all the gradients (backpropagation)\n",
    "        loss.backward()\n",
    "        #backward DOES not adjust the weight Yet, just backpropagation\n",
    "        #We want to calculate the gradients of all the parameters (8-6 weights and 2 bias)\n",
    "        #IN RESPECT TO the LOSS... dJ/dw11, dJ/dw2, dJ/dw13..., dJ/db1, dJ/db2\n",
    "\n",
    "        #4. Update the parameters using the optim\n",
    "        # W = W - alpha * gradient - no need to do this here\n",
    "        optim.step() # optim already kow the learning rate and has the parameters\n",
    "\n",
    "        print(f\"Epoch: {epoch} - Loss: {loss}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inference / Testing\n",
    "\n",
    "Test some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([73., 67., 43.]), tensor([56., 70.]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a np array of \n",
    "# [74, 68, 42] , [92, 88, 65]\n",
    "\n",
    "X_test_np = np.array([[74., 68., 42.],[92., 88., 65.]], dtype='float32')\n",
    "\n",
    "\n",
    "# Please make a tensor\n",
    "X_test = torch.tensor(X_test_np)\n",
    "\n",
    "# Then use our model to predic the number of orenges and apples\n",
    "yhat = model(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 59.8395,  75.8474],\n",
       "        [ 84.1226, 104.9896]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 56.,  70.],\n",
       "        [ 81., 101.]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the loss comparing ds[0] and ds[1]\n",
    "ytest = ds[0:2][1]\n",
    "ytest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(18.6502, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = J_fn(yhat, ytest)\n",
    "print(loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.venv': pipenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "27768773b483d82a9b2b839e3fa80b1be5789db7fd78df4eedef2df266871616"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
