{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")\n",
    "print(\"Device: \", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plan for today:\n",
    "\n",
    "1. ETL \n",
    "   1. Specifying some some random input\n",
    "   2. PyTorch Dataset and DataLoader\n",
    "2. EDA - we gonna just skip because we are lazy...\n",
    "3. Feature Engineering / Cleaning - which we don't need to....\n",
    "4. Modeling \n",
    "   1. `nn.Linear` (luckily, you already understand this!  Yay!)\n",
    "   2. Define loss function (mse for regression, cross entrophy for classification)\n",
    "   3. Define the optimizer function (gradient descent ; adam)\n",
    "   4. Train the model\n",
    "5. Inference / Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider this data:\n",
    "\n",
    "<img src = \"figures/japan.png\" width=\"400\">\n",
    "\n",
    "In a linear regression model, each target variable is estimated to be a weighted sum of the input variables, offset by some constant, known as a bias :\n",
    "\n",
    "$$\\text{yield}_\\text{apple}  = w_{11} * \\text{temp} + w_{12} * \\text{rainfall} + w_{13} * \\text{humidity} + b_{1}$$\n",
    "\n",
    "$$\\text{yield}_\\text{orange} = w_{21} * \\text{temp} + w_{22} * \\text{rainfall} + w_{23} * \\text{humidity} + b_{2}$$\n",
    "\n",
    "Visually, it means that the yield of apples is a linear or planar function of temperature, rainfall and humidity:\n",
    "\n",
    "<img src = \"figures/japan2.png\" width=\"400\">\n",
    "\n",
    "The learning part of linear regression is to figure out a set of weights <code>w11, w12,... w23, b1 \\& b2</code> using gradient descent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X(temp, rainfall, hum)\n",
    "\n",
    "X_train = np.array([[73, 67, 43], [91, 88, 64], [87, 134, 58], \n",
    "                   [102, 43, 37], [69, 96, 70], [73, 67, 43], \n",
    "                   [91, 88, 64], [87, 134, 58], [102, 43, 37], \n",
    "                   [69, 96, 70], [73, 67, 43], [91, 88, 64], \n",
    "                   [87, 134, 58], [102, 43, 37], [69, 96, 70]], \n",
    "                  dtype='float32')\n",
    "\n",
    "# Targets (apples, oranges)\n",
    "Y_train = np.array([[56, 70], [81, 101], [119, 133], \n",
    "                    [22, 37], [103, 119], [56, 70], \n",
    "                    [81, 101], [119, 133], [22, 37], \n",
    "                    [103, 119], [56, 70], [81, 101], \n",
    "                    [119, 133], [22, 37], [103, 119]], \n",
    "                   dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15, 3])\n",
      "torch.Size([15, 2])\n"
     ]
    }
   ],
   "source": [
    "#Create tensors from the np. arr.\n",
    "\n",
    "inputs = torch.tensor(X_train)\n",
    "targets = torch.tensor(Y_train)\n",
    "\n",
    "#Print the shape\n",
    "print(inputs.shape)\n",
    "print(targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Dataset\n",
    "\n",
    "- We are going to crate a TensorDataset on top of these tensors, so we can access each row from the input and target tuples.\n",
    "\n",
    "- Note - This is neded if we want to use the DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X.shape (m,n) Y.sahpe (m,k)\n",
    "ds = TensorDataset(inputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([91., 88., 64.]), tensor([ 81., 101.]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the format PyTorch wants\n",
    "# A tuple of two tensors, the x and coresponding  y\n",
    "ds[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([91., 88., 64.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 DataLoader\n",
    "\n",
    "- By default PyTorch works in batch.\n",
    "\n",
    "- In simple words it will ALWAYS take some mini-batch and performs gradient descent\n",
    "\n",
    "- Mini-batch - beacuse assume you won't be able be able to fit in 1 M samples into GPU RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will automatically createsa an enumarator , look at each batch\n",
    "# Means you can simply perform a for loop onto DataLoader\n",
    "# If DataLoader is not used we have to manually selectthe mini-batch\n",
    "# Randomized\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 3 # Can be any number\n",
    "# Too small - slow\n",
    "# Too large - run out of memory\n",
    "dl = DataLoader(ds, batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[69., 96., 70.],\n",
      "        [91., 88., 64.],\n",
      "        [73., 67., 43.]]), tensor([[103., 119.],\n",
      "        [ 81., 101.],\n",
      "        [ 56.,  70.]])]\n"
     ]
    }
   ],
   "source": [
    "# Now this dl is basically an enumerator, in which we can loop on....\n",
    "for something in dl:\n",
    "    print(something)   # Three X+Y tuples\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X : tensor([[102.,  43.,  37.],\n",
      "        [ 87., 134.,  58.],\n",
      "        [ 73.,  67.,  43.]])\n",
      "Y : tensor([[ 22.,  37.],\n",
      "        [119., 133.],\n",
      "        [ 56.,  70.]])\n"
     ]
    }
   ],
   "source": [
    "for x, y in dl:\n",
    "    print(f\"X : {x}\")\n",
    "    print(f\"Y : {y}\")\n",
    "    break\n",
    "\n",
    "# The dl keeps an internal counter\n",
    "# This dl is keep on running : which is intentional, we have the concept of \"epochs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Define our neural network\n",
    "\n",
    "- How many layers we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "#Define our NN here\n",
    "#Just one layer at the moment\n",
    "# Later add one more layer\n",
    "# Format nn.Linear(in_features, out_features)\n",
    "# Format nn.Linear(temp;rainfalll;hum, oranges;apples)\n",
    "model = nn.Linear(3,2) # <-- Hidden later\n",
    " \n",
    "# Linerar layers are simple matrix multiplication...\n",
    "# Many other names, In Keras we called Dense, In TensorFlow we called FullyConnected\n",
    "# Keras very high level, not good for research, devlopment \n",
    "# TensorFlow is developed by Google, it is quite good, \n",
    "\n",
    "# Fore very huge complex high performance models TensorFlow is much better, optimized\n",
    "# Very low-level than Pytorch\n",
    "#For very general almost any model that we use in even un reserach - PyTorch is much beteer\n",
    "# Due to its computational graph\n",
    "\n",
    "# TensorFlow something called TensorFlowLite which is the way.\n",
    "# You want to use for mobile phones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I wonder whether having one extra layer will reduce the loss.\n",
    "\n",
    "# model = nn.Sequential(\n",
    "#     nn.Linear()\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclass AnyNameCapitalized(nn.Module): # It is basically inherin nn.Module\\n    def __init()__:\\n        super(.__init()__ #super is pasically inheriting nn.Module init\\n        #We define the layers here\\n    \\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Class is the best practice for creating a neural network\n",
    "\n",
    "#format\n",
    "'''\n",
    "class AnyNameCapitalized(nn.Module): # It is basically inherin nn.Module\n",
    "    def __init()__:\n",
    "        super(.__init()__ #super is pasically inheriting nn.Module init\n",
    "        #We define the layers here\n",
    "    \n",
    "'''\n",
    "\n",
    "# class NeuralNetwork(nn.Module):\n",
    "    \n",
    "#     def __init__(self, input_size, hidden_size, output_size):\n",
    "#         super.__init__()\n",
    "#         self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "#         self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         out = self.fc1(x)\n",
    "#         out = self.fc2(out)\n",
    "#         return out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = NeuralNetwork (3, 99, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.3747, -0.4138,  0.2620],\n",
       "        [ 0.4578,  0.5225,  0.3344]], requires_grad=True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weight # By default these weights are uniformly close to 0\n",
    "# model.fc1.weight\n",
    "# model.fc2.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weight.shape #This one is basically in the shape (out_feature, in_feature)\n",
    "\n",
    "# You can imagine X @ W^T\n",
    "# After you transpose W, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([ 0.1390, -0.0138], requires_grad=True)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.bias\n",
    "\n",
    "# Why two bias, beacause y1 bias and y2 bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.3747, -0.4138,  0.2620],\n",
       "         [ 0.4578,  0.5225,  0.3344]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.1390, -0.0138], requires_grad=True)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#p.numel() just flattern everythin\n",
    "sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Why 8 - 6 weights and 2 bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15, 3])\n",
      "tensor([[-43.6687,  82.7947],\n",
      "        [-53.6004, 109.0306],\n",
      "        [-72.7071, 129.2273],\n",
      "        [-46.1749,  81.5259],\n",
      "        [-47.0964, 105.1445],\n",
      "        [-43.6687,  82.7947],\n",
      "        [-53.6004, 109.0306],\n",
      "        [-72.7071, 129.2273],\n",
      "        [-46.1749,  81.5259],\n",
      "        [-47.0964, 105.1445],\n",
      "        [-43.6687,  82.7947],\n",
      "        [-53.6004, 109.0306],\n",
      "        [-72.7071, 129.2273],\n",
      "        [-46.1749,  81.5259],\n",
      "        [-47.0964, 105.1445]], grad_fn=<AddmmBackward0>)\n",
      "torch.Size([15, 2])\n"
     ]
    }
   ],
   "source": [
    "print(inputs.shape)\n",
    "\n",
    "output = model(inputs) # (15,3) @ (3, 2) = (15, 2)\n",
    "\n",
    "print(output)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Define the loss fn.\n",
    "\n",
    "- Should ber MSE or Cross Entrophy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Under the nn mudule , there are many loss function\n",
    "\n",
    "J_fn = nn.MSELoss()\n",
    "\n",
    "#Later on you will know how to use this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Define the optimizer\n",
    "1. Predict\n",
    "2. Loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normally in skleran we call fit, it will perform gradient descent\n",
    "# In code from scratch we need to like specify how we want to update the gradients\n",
    "# Optimizer handles how we update the parameters\n",
    "# If we use w = w -alpha (gradient) ==> gradient descent\n",
    "#Stochastic gradient descent ==>  is not one sample - mini-batch\n",
    "\n",
    "optim = torch.optim.SGD(model.parameters(), lr=0.0001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Actually train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (4247381336.py, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [26]\u001b[0;36m\u001b[0m\n\u001b[0;31m    #4. update the parameters using the optim\u001b[0m\n\u001b[0m                                             ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5 # It depends ... trian and error...\n",
    "def fit():\n",
    "    #For num_epochs\n",
    "        #for dataloader\n",
    "         #1. predict(forward pass )\n",
    "         #2. calculate loss\n",
    "         #3. calculate gradient\n",
    "         #4. update the parameters using the optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 - Loss: 8678.7001953125\n",
      "Epoch: 0 - Loss: 4226.14892578125\n",
      "Epoch: 0 - Loss: 1529.9422607421875\n",
      "Epoch: 0 - Loss: 90.0040283203125\n",
      "Epoch: 0 - Loss: 136.44259643554688\n",
      "Epoch: 1 - Loss: 335.1781921386719\n",
      "Epoch: 1 - Loss: 312.5815124511719\n",
      "Epoch: 1 - Loss: 447.3495178222656\n",
      "Epoch: 1 - Loss: 377.1439514160156\n",
      "Epoch: 1 - Loss: 1095.5439453125\n",
      "Epoch: 2 - Loss: 795.25146484375\n",
      "Epoch: 2 - Loss: 509.3713684082031\n",
      "Epoch: 2 - Loss: 421.5940246582031\n",
      "Epoch: 2 - Loss: 270.5837097167969\n",
      "Epoch: 2 - Loss: 393.8300476074219\n",
      "Epoch: 3 - Loss: 943.9540405273438\n",
      "Epoch: 3 - Loss: 696.36181640625\n",
      "Epoch: 3 - Loss: 720.5914916992188\n",
      "Epoch: 3 - Loss: 362.7292785644531\n",
      "Epoch: 3 - Loss: 196.3837127685547\n",
      "Epoch: 4 - Loss: 100.54447174072266\n",
      "Epoch: 4 - Loss: 30.181947708129883\n",
      "Epoch: 4 - Loss: 35.1323127746582\n",
      "Epoch: 4 - Loss: 19.931486129760742\n",
      "Epoch: 4 - Loss: 32.45169448852539\n",
      "Epoch: 5 - Loss: 125.30400848388672\n",
      "Epoch: 5 - Loss: 205.1574249267578\n",
      "Epoch: 5 - Loss: 260.8034973144531\n",
      "Epoch: 5 - Loss: 147.45262145996094\n",
      "Epoch: 5 - Loss: 38.327693939208984\n",
      "Epoch: 6 - Loss: 63.18210983276367\n",
      "Epoch: 6 - Loss: 40.80220413208008\n",
      "Epoch: 6 - Loss: 12.350150108337402\n",
      "Epoch: 6 - Loss: 7.06097412109375\n",
      "Epoch: 6 - Loss: 44.32277297973633\n",
      "Epoch: 7 - Loss: 149.47727966308594\n",
      "Epoch: 7 - Loss: 217.33184814453125\n",
      "Epoch: 7 - Loss: 157.6637725830078\n",
      "Epoch: 7 - Loss: 19.7841739654541\n",
      "Epoch: 7 - Loss: 15.14720630645752\n",
      "Epoch: 8 - Loss: 16.556589126586914\n",
      "Epoch: 8 - Loss: 5.738042831420898\n",
      "Epoch: 8 - Loss: 29.792184829711914\n",
      "Epoch: 8 - Loss: 21.064273834228516\n",
      "Epoch: 8 - Loss: 12.660842895507812\n",
      "Epoch: 9 - Loss: 10.626709938049316\n",
      "Epoch: 9 - Loss: 5.515343189239502\n",
      "Epoch: 9 - Loss: 20.085826873779297\n",
      "Epoch: 9 - Loss: 16.60826873779297\n",
      "Epoch: 9 - Loss: 11.838160514831543\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10 # It depends ... trian and error...\n",
    "for epoch in range(num_epochs):\n",
    "    #for dataloader\n",
    "    for x, y in dl: #What is the shape of x and y here\n",
    "        #x and y are the size of the mini-batch of X_train and y_train\n",
    "        #batch size is 3\n",
    "        # x (3,3) y (3,2)\n",
    "        # X: (batch, feature) = (3,3)\n",
    "        # y: (batch, target) = (3,2)\n",
    "        x.to(device)# Device is either cpu or cuda\n",
    "        y.to(device)\n",
    "\n",
    "        #1. Predict (forward pass)\n",
    "        yhat = model(x)\n",
    "\n",
    "        #2. Calculate loss\n",
    "        #format J_fn(inputs, targets)\n",
    "        loss = J_fn(yhat, y)\n",
    "\n",
    "        #3. Calculate gradient\n",
    "        #3.1 clear out the previous gradients\n",
    "        optim.zero_grad()\n",
    "        #3.2 Call backwars() on loss to retrive all the gradients (backpropagation)\n",
    "        loss.backward()\n",
    "        #backward DOES not adjust the weight Yet, just backpropagation\n",
    "        #We want to calculate the gradients of all the parameters (8-6 weights and 2 bias)\n",
    "        #IN RESPECT TO the LOSS... dJ/dw11, dJ/dw2, dJ/dw13..., dJ/db1, dJ/db2\n",
    "\n",
    "        #4. Update the parameters using the optim\n",
    "        # W = W - alpha * gradient - no need to do this here\n",
    "        optim.step() # optim already kow the learning rate and has the parameters\n",
    "\n",
    "        print(f\"Epoch: {epoch} - Loss: {loss}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inference / Testing\n",
    "\n",
    "Test some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([73., 67., 43.]), tensor([56., 70.]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a np array of \n",
    "# [74, 68, 42] , [92, 88, 65]\n",
    "\n",
    "X_test_np = np.array([[74., 68., 42.],[92., 88., 65.]], dtype='float32')\n",
    "\n",
    "\n",
    "# Please make a tensor\n",
    "X_test = torch.tensor(X_test_np)\n",
    "\n",
    "# Then use our model to predic the number of orenges and apples\n",
    "yhat = model(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[58.9810, 67.1418],\n",
       "        [86.7383, 93.6330]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 56.,  70.],\n",
       "        [ 81., 101.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the loss comparing ds[0] and ds[1]\n",
    "ytest = ds[0:2][1]\n",
    "ytest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(18.6502, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = J_fn(yhat, ytest)\n",
    "print(loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "27768773b483d82a9b2b839e3fa80b1be5789db7fd78df4eedef2df266871616"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
