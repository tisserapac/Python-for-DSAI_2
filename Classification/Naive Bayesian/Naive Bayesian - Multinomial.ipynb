{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming for Data Science and Artificial Intelligence\n",
    "\n",
    "## Naive Bayesian - Multinomial\n",
    "\n",
    "### Readings: \n",
    "- [VANDER] Ch5\n",
    "- [HASTIE] Ch6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Naive Classification\n",
    "\n",
    "The Gaussian assumption just described is by no means the only simple assumption that could be used to specify the generative distribution for each label.  Another useful example is multinomial naive bayes, where the features are assumed to be generated from a simple multinomial distribution.  **The multinomial distribution describes the probability of observing counts among a number of categories, and thus multinomial naive bayes is most appropriate for features that represent counts or count rates.**\n",
    "\n",
    "The idea is precisely the same as before, except that instead of modeling the data distribution with the best-fit Gaussian, we model the data distribuiton with a best-fit multinomial distribution.\n",
    "\n",
    "One place where multinomial naive Bayes is often used is in **text classification**, where the features $w$ are related to word counts or frequencies within the documents to be classified and $y$ will be our class.  The formation is as follows:\n",
    "\n",
    "$$\n",
    "P(y|w) = \\frac{P(w|y)P(y)}{P(w)}\n",
    "$$\n",
    "\n",
    "**Implementation steps**: \n",
    "\n",
    "1. Prepare your data\n",
    "    - $\\mathbf{X}$ and $\\mathbf{y}$ in the right shape\n",
    "        - $\\mathbf{X}$ -> $(m, n)$\n",
    "        - $\\mathbf{y}$ -> $(m,  )$\n",
    "        - Note that theta is not needed.  Why?\n",
    "    - train-test split\n",
    "    - feature scale\n",
    "    - clean out any missing data\n",
    "    - (optional) feature engineering\n",
    "\n",
    "2. Using the train documents, calculate the **likelihoods** of each word.  Following multinomial distribution, for a given word $w_i$, we count how many of $w_i$ belong in class $k$, we then divide this by the count of all the words that belong to $k$. This gives us the conditional probability for a word $w$ given $k$:\n",
    "\n",
    "    $$ P(w_i \\in train \\mid y=k) = \\frac{count(w_i \\in train, k)}{\\sum_{i=1}^{n} count(w_i \\in train, k)} $$\n",
    "    \n",
    "where \n",
    "\n",
    "$n$ stands for number of unique vocabulary (i.e., features) and $m$ stands for number of documents (i.e., samples). \n",
    "\n",
    "Example:\n",
    "\n",
    "| | docID  | words in doc    | China?   |    \n",
    "|---:|:-------------|:-----------|:------|\n",
    "| Training set | 1  | Chinese Beijing Chinese       | Yes   |\n",
    "|  | 2  | Chinese Chinese Shanghai    | Yes  |\n",
    "|  | 3  | Chinese Macao    | Yes   |\n",
    "|  | 4  | Tokyo Japan Chinese   | No   |\n",
    "| Test set | 5  | Chinese Chinese Chinese Tokyo Japan    | ?   |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's define some random dataset\n",
    "train = np.array([\n",
    "    'Chinese Beijing Chinese',\n",
    "    'Chinese Chinese Shanghai',\n",
    "    'Chinese Macao',\n",
    "    'Tokyo Japan Chinese',\n",
    "])\n",
    "\n",
    "test = np.array([\n",
    "    'Chinese Chinese Chinese Tokyo Japan'\n",
    "])\n",
    "\n",
    "train_target = np.array([1, 1, 1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names:  ['beijing', 'chinese', 'japan', 'macao', 'shanghai', 'tokyo']\n",
      "Type:  <class 'scipy.sparse._csr.csr_matrix'>\n",
      "[[1 2 0 0 0 0]\n",
      " [0 2 0 0 1 0]\n",
      " [0 1 0 1 0 0]\n",
      " [0 1 1 0 0 1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chaklam/DSAI/Environments/teaching_env/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#let's use some library to count for us\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X_train = vectorizer.fit_transform(train)\n",
    "print(\"Feature names: \", vectorizer.get_feature_names())\n",
    "print(\"Type: \", type(X_train))\n",
    "print(X_train.toarray())  #the default is a sparse matrix, use toarray() to see it in the familiar numpy form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the likelihood is very easy, i.e., count of each word belonging to that class / total count of words belonging to that class\n",
    "def likelihood(X_class):\n",
    "    return ((X_class.sum(axis=0))) / (np.sum(X_class.sum(axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names: \n",
      " ['beijing', 'chinese', 'japan', 'macao', 'shanghai', 'tokyo']\n",
      "====When y = 1=====\n",
      "X_train\n",
      " [[1 2 0 0 0 0]\n",
      " [0 2 0 0 1 0]\n",
      " [0 1 0 1 0 0]]\n",
      "Count of each word:  [[1 5 0 1 1 0]]\n",
      "Total count of words:  8\n",
      "P(X_train | y = 1): \n",
      " [[0.125 0.625 0.    0.125 0.125 0.   ]]\n",
      "====When y = 0=====\n",
      "X_train\n",
      " [[0 1 1 0 0 1]]\n",
      "Count of each word:  [[0 1 1 0 0 1]]\n",
      "Total count of words:  3\n",
      "P(X_train | y = 0): \n",
      " [[0.         0.33333333 0.33333333 0.         0.         0.33333333]]\n"
     ]
    }
   ],
   "source": [
    "X_train_class1 = X_train[train_target == 1] \n",
    "X_train_class0 = X_train[train_target == 0]\n",
    "\n",
    "print(\"Feature names: \\n\", vectorizer.get_feature_names())\n",
    "\n",
    "print(\"====When y = 1=====\")\n",
    "print(\"X_train\\n\", X_train_class1.toarray())\n",
    "print(\"Count of each word: \", X_train_class1.sum(axis=0))\n",
    "print(\"Total count of words: \", np.sum(X_train_class1.sum(axis=0)))\n",
    "\n",
    "likelihood1 = likelihood(X_train_class1)\n",
    "print(\"P(X_train | y = 1): \\n\", likelihood1)\n",
    "#P(word    | y = 1) = count of word / total words\n",
    "#P(beijing | y = 1) = 1 / 8 = 0.125\n",
    "#P(chinese | y = 1) = 5 / 8 = 0.625\n",
    "#P(japan   | y = 1) = 0 / 8 = 0\n",
    "#P(macao   | y = 1) = 1 / 8 = 0.125\n",
    "#P(shanghai| y = 1) = 1 / 8 = 0.125\n",
    "#P(tokyo   | y = 1) = 0 / 8 = 0.125\n",
    "\n",
    "print(\"====When y = 0=====\")\n",
    "print(\"X_train\\n\", X_train_class0.toarray())\n",
    "print(\"Count of each word: \", X_train_class0.sum(axis=0))\n",
    "print(\"Total count of words: \", np.sum(X_train_class0.sum(axis=0)))\n",
    "\n",
    "likelihood0 = likelihood(X_train_class0)\n",
    "print(\"P(X_train | y = 0): \\n\", likelihood0)\n",
    "#P(beijing | y = 0) = 0 / 3 = 0\n",
    "#P(chinese | y = 0) = 1 / 3 = 0.33\n",
    "#P(japan   | y = 0) = 1 / 3 = 0.33\n",
    "#P(macao   | y = 0) = 0 / 3 = 0\n",
    "#P(shanghai| y = 0) = 0 / 3 = 0\n",
    "#P(tokyo   | y = 0) = 1 / 3 = 0.33\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Since nothing in this world has zero probability, similarly, even we never see a particular word in some class should not gaurantee a zero probability, thus we can perform **Laplace smoothing** to account for any words with zero count.  Also zero probability is not good when we do a product of probabilities.\n",
    "\n",
    "    $$ P(w_i \\in train \\mid y=k) = \\frac{count(w_i \\in train, k) + 1}{\\sum_{i=1}^{n} count(w_i \\in train, k) + n} $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood(X_class, laplace=1):\n",
    "    return ((X_class.sum(axis=0)) + laplace) / (np.sum(X_class.sum(axis=0) + laplace))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(X_train | y = 1) with laplace: \n",
      " [[0.143 0.429 0.071 0.143 0.143 0.071]]\n",
      "P(X_train | y = 0) with laplace: \n",
      " [[0.111 0.222 0.222 0.111 0.111 0.222]]\n"
     ]
    }
   ],
   "source": [
    "#rerun the new likelihood function\n",
    "likelihood1 = likelihood(X_train_class1)\n",
    "likelihood0 = likelihood(X_train_class0)\n",
    "\n",
    "print(\"P(X_train | y = 1) with laplace: \\n\", np.round(likelihood1, 3))\n",
    "#P(beijing | y = 1) = (1 + 1) / (8 + 6) = 0.143\n",
    "#P(chinese | y = 1) = (5 + 1) / (8 + 6) = 0.429\n",
    "#P(japan   | y = 1) = (0 + 1) / (8 + 6) = 0.071\n",
    "#P(macao   | y = 1) = (1 + 1) / (8 + 6) = 0.143\n",
    "#P(shanghai| y = 1) = (1 + 1) / (8 + 6) = 0.143\n",
    "#P(tokyo   | y = 1) = (0 + 1) / (8 + 6) = 0.071\n",
    "\n",
    "print(\"P(X_train | y = 0) with laplace: \\n\", np.round(likelihood0, 3))\n",
    "#P(beijing | y = 0) = (0 + 1) / (3 + 6) = 0.11\n",
    "#P(chinese | y = 0) = (1 + 1) / (3 + 6) = 0.22\n",
    "#P(japan   | y = 0) = (1 + 1) / (3 + 6) = 0.22\n",
    "#P(macao   | y = 0) = (0 + 1) / (3 + 6) = 0.11\n",
    "#P(shanghai| y = 0) = (0 + 1) / (3 + 6) = 0.11\n",
    "#P(tokyo   | y = 0) = (1 + 1) / (3 + 6) = 0.22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Find **priors** $P(y)$ where is simply number of documents belonging to that class divided by all documents\n",
    "\n",
    "$$P(y = k) = \\frac{\\Sigma_{i=1}^{m}1(y=k)}{m} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target:  [1 1 1 0]\n",
      "Prior 1 (P(y=1)):  0.75\n",
      "Prior 0 (P(y=0)):  0.25\n"
     ]
    }
   ],
   "source": [
    "prior1 = len(train_target[train_target==1])/len(train_target)\n",
    "prior0 = len(train_target[train_target==0])/len(train_target)\n",
    "\n",
    "print(\"Target: \", train_target)\n",
    "print(\"Prior 1 (P(y=1)): \", prior1)\n",
    "print(\"Prior 0 (P(y=0)): \", prior0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Once we get the **likelihoods** from the train data.  If given some test data, we simply use this likelihood to calculate the total likelihood of the test document.  Similarly, since we have more than one word in the test document, we need to make a product of all likelihood of each word in the test document.    \n",
    "    $$ P(w \\in test \\mid y=k) = \\prod_{i=1}^{n} P(w_i \\in test \\mid y=k)^{\\text{freq of }w_i \\in test}$$\n",
    "    \n",
    "Then we can multiply $P(y)P(w \\in test \\mid y)$ for each class which will give us $P (y \\mid x)$ (**posteriors**)\n",
    "\n",
    "$$P (y \\mid x) = P(y=k)\\prod_{i=1}^{n} P(w_i \\in test \\mid y=k)^{\\text{freq of }w_i \\in test}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names:  ['beijing', 'chinese', 'japan', 'macao', 'shanghai', 'tokyo']\n",
      "Type:  <class 'scipy.sparse._csr.csr_matrix'>\n",
      "[[0 3 1 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "#let's first recall what is our test dataset\n",
    "#also transform to the count matrix, using the training set count model (thus, only transform, no \"fit\")\n",
    "\n",
    "test = np.array([\n",
    "    'Chinese Chinese Chinese Tokyo Japan'\n",
    "])\n",
    "\n",
    "X_test = vectorizer.transform(test)\n",
    "print(\"Feature names: \", vectorizer.get_feature_names())\n",
    "print(\"Type: \", type(X_test))\n",
    "print(X_test.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names: \n",
      " ['beijing', 'chinese', 'japan', 'macao', 'shanghai', 'tokyo']\n",
      "Likelihood 1:  [[0.14285714 0.42857143 0.07142857 0.14285714 0.14285714 0.07142857]]\n",
      "Likelihood 0:  [[0.11111111 0.22222222 0.22222222 0.11111111 0.11111111 0.22222222]]\n",
      "P(X_test | y = 1):  0.0004016183732968405\n",
      "P(X_test | y = 0):  0.0005419228098697689\n"
     ]
    }
   ],
   "source": [
    "#let's first print the likelihoods we get from the training set\n",
    "print(\"Feature names: \\n\", vectorizer.get_feature_names())\n",
    "print(\"Likelihood 1: \", likelihood1)\n",
    "print(\"Likelihood 0: \", likelihood0)\n",
    "\n",
    "pxtest_y1 = np.prod(np.power(likelihood1, X_test.toarray()))\n",
    "# = 0.142^0 * 0.428^3 * 0.07^1 * 0.142^0 * 0.142^0 * 0.07^1\n",
    "pxtest_y0 = np.prod(np.power(likelihood0, X_test.toarray()))\n",
    "# = 0.11^0 * 0.22^3 * 0.22^1 * 0.11^0 * 0.11^0 * 0.22^1\n",
    "\n",
    "print(\"P(X_test | y = 1): \", pxtest_y1)\n",
    "print(\"P(X_test | y = 0): \", pxtest_y0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(y=1|X_test) = p(y=1)p(X_test|y=1):  0.00030121377997263036\n",
      "P(y=0|X_test) = p(y=0)p(X_test|y=0):  0.00013548070246744223\n"
     ]
    }
   ],
   "source": [
    "#last, we can multiply with the prior to get the posterior (p(y|x))\n",
    "py1_x = prior1 * pxtest_y1\n",
    "py0_x = prior0 * pxtest_y0\n",
    "\n",
    "print(\"P(y=1|X_test) = p(y=1)p(X_test|y=1): \", py1_x)\n",
    "print(\"P(y=0|X_test) = p(y=0)p(X_test|y=0): \", py0_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Instead of probabilities, we gonna use log (base e) probabiities which have several benefits:\n",
    "    - **Speed** - Log probabilities become addition, which is faster than multiplication\n",
    "    - **Stability** - Probabilities can be too small where some significant digits can be lost during calculations. Log probabiities can prevent such underflow.  If you don't believe me, try perform $\\log_e(0.0000001)$\n",
    "    - **Simplicity** - Many distributions have exponential form.  Taking log cancels out the exp.  The reason we can apply $\\log$ is because $\\log$ is a monotically increasing function, thus will not alter the result \n",
    "    - **Dot product** - After log, addition can often expressed as dot product of matrix, simplifying the code implementation\n",
    "    \n",
    "   Now that you are convinced, \n",
    "   \n",
    "   $$P(y=k)\\prod_{i=1}^{n} p(w_i \\in test \\mid y=k)^{\\text{freq of }w_i \\in test}$$  becomes\n",
    "   \n",
    "  $$\\log \\ P(y=k) + (\\text{freq of }w_i \\in test) * \\sum_{i=1}^{n} \\log \\ p(w_i \\in test \\mid y=k)$$\n",
    "  - Note 1: Log of multiplication becomes addition\n",
    "  - Note 2: Exponent of log becomes multiplicative scalar\n",
    "  \n",
    " Thus, in implementation we can expressed as\n",
    "\n",
    "<code>np.log(priors) + X_test @ np.log(likelihoods.T) </code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(X_test | y = 1):  [[-7.82000824]]\n",
      "P(X_test | y = 0):  [[-7.52038698]]\n"
     ]
    }
   ],
   "source": [
    "#let's recompute using the log way\n",
    "pxtest_y1 = X_test.toarray() @ np.log(likelihood1.T)\n",
    "pxtest_y0 = X_test.toarray() @ np.log(likelihood0.T)\n",
    "\n",
    "print(\"P(X_test | y = 1): \", pxtest_y1)\n",
    "print(\"P(X_test | y = 0): \", pxtest_y0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(y=1|X_test) = p(y=1)p(X_test|y=1):  [[2.24967618]]\n",
      "P(y=0|X_test) = p(y=0)p(X_test|y=0):  [[10.42547007]]\n"
     ]
    }
   ],
   "source": [
    "py1_x = np.log(prior1) * pxtest_y1\n",
    "py0_x = np.log(prior0) * pxtest_y0\n",
    "\n",
    "print(\"P(y=1|X_test) = p(y=1)p(X_test|y=1): \", py1_x)\n",
    "print(\"P(y=0|X_test) = p(y=0)p(X_test|y=0): \", py0_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting them together\n",
    "\n",
    "#### 1. Prepare some data\n",
    "\n",
    "Here we will use the sparse word count features from the 20 Newsgroups corpus to show how we might classify these short documents into categories.\n",
    "\n",
    "Let's download the data and take a look at the target names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "data = fetch_20newsgroups()\n",
    "data.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, for simplicity here, we will select just a few of these categories, and download the training and testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['talk.religion.misc', 'soc.religion.christian',\n",
    "              'sci.space', 'comp.graphics']\n",
    "train = fetch_20newsgroups(subset='train', categories=categories)\n",
    "test = fetch_20newsgroups(subset='test', categories=categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last, we will print some example data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: jono@mac-ak-24.rtsg.mot.com (Jon Ogden)\n",
      "Subject: Re: Losing your temper is not a Christian trait\n",
      "Organization: Motorola LPA Development\n",
      "Lines: 26\n",
      "\n",
      "In article <Apr.23.02.55.47.1993.3138@geneva.rutgers.edu>, jcj@tellabs.com\n",
      "(jcj) wrote:\n",
      "\n",
      "> I'd like to remind people of the withering of the fig tree and Jesus\n",
      "> driving the money changers et. al. out of the temple.  I think those\n",
      "> were two instances of Christ showing anger (as part of His human side).\n",
      "> \n",
      "Yes, and what about Paul saying:\n",
      "\n",
      "26 Be ye angry, and sin not: let not the sun go down upon your wrath:\n",
      "(Ephesians 4:26).\n",
      "\n",
      "Obviously then, we can be angry w/o sinning.\n",
      "\n",
      "Jon\n",
      "\n",
      "------------------------------------------------\n",
      "Jon Ogden         - jono@mac-ak-24.rtsg.mot.com\n",
      "Motorola Cellular - Advanced Products Division\n",
      "Voice: 708-632-2521      Data: 708-632-6086\n",
      "------------------------------------------------\n",
      "\n",
      "They drew a circle and shut him out.\n",
      "Heretic, Rebel, a thing to flout.\n",
      "But Love and I had the wit to win;\n",
      "We drew a circle and took him in.\n",
      "\n",
      "Target:  2\n"
     ]
    }
   ],
   "source": [
    "print(train.data[0]) #first 300 words\n",
    "print(\"Target: \", train.target[0])  #start with 1, soc.religion.christian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 Transform our data to frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:    (0, 14822)\t1\n",
      "  (0, 18774)\t2\n",
      "  (0, 20502)\t2\n",
      "  (0, 4688)\t2\n",
      "  (0, 1572)\t2\n",
      "  (0, 27916)\t2\n",
      "  (0, 21988)\t2\n",
      "  (0, 9095)\t3\n",
      "  (0, 18765)\t3\n",
      "  (0, 23326)\t2\n",
      "  (0, 30617)\t1\n",
      "  (0, 26562)\t1\n",
      "  (0, 20308)\t1\n",
      "  (0, 35183)\t2\n",
      "  (0, 31463)\t1\n",
      "  (0, 18370)\t1\n",
      "  (0, 22932)\t3\n",
      "  (0, 8590)\t1\n",
      "  (0, 32244)\t1\n",
      "  (0, 23591)\t1\n",
      "  (0, 22013)\t2\n",
      "  (0, 20366)\t1\n",
      "  (0, 11327)\t1\n",
      "  (0, 20065)\t1\n",
      "  (0, 1655)\t3\n",
      "  :\t:\n",
      "  (0, 8236)\t1\n",
      "  (0, 4416)\t1\n",
      "  (0, 25675)\t1\n",
      "  (0, 11891)\t1\n",
      "  (0, 33990)\t1\n",
      "  (0, 2879)\t2\n",
      "  (0, 2691)\t2\n",
      "  (0, 1631)\t1\n",
      "  (0, 10677)\t1\n",
      "  (0, 2645)\t1\n",
      "  (0, 31737)\t1\n",
      "  (0, 12166)\t2\n",
      "  (0, 8666)\t2\n",
      "  (0, 29122)\t1\n",
      "  (0, 16610)\t2\n",
      "  (0, 16493)\t1\n",
      "  (0, 26646)\t1\n",
      "  (0, 31753)\t1\n",
      "  (0, 14421)\t1\n",
      "  (0, 7576)\t1\n",
      "  (0, 20341)\t1\n",
      "  (0, 16025)\t1\n",
      "  (0, 34678)\t1\n",
      "  (0, 34604)\t1\n",
      "  (0, 32091)\t1\n",
      "y_train:  2\n"
     ]
    }
   ],
   "source": [
    "#transform our X to frequency data\n",
    "vectorizer = CountVectorizer()\n",
    "X_train = vectorizer.fit_transform(train.data)\n",
    "X_test = vectorizer.transform(test.data)\n",
    "X_test = X_test.toarray()  #vectorizer gives us a sparse matrix; convert back to dense matrix\n",
    "\n",
    "y_train = train.target\n",
    "y_test = test.target\n",
    "\n",
    "print(\"X_train: \", X_train[0])\n",
    "print(\"y_train: \", y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Calculating likelihood anrd prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood(X_class, laplace=1):\n",
    "    return ((X_class.sum(axis=0)) + laplace) / (np.sum(X_class.sum(axis=0) + laplace))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prior(X_class, m):\n",
    "    return X_class.shape[0] / m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(X_train, y_train):\n",
    "    m, n = X_train.shape\n",
    "    classes = np.unique(y_train)  #list of class\n",
    "    k = len(classes) #number of class\n",
    "    \n",
    "    priors = np.zeros(k) #prior for each classes\n",
    "    likelihoods = np.zeros((k, n)) #likehood for each class of each feature\n",
    "    \n",
    "    for idx, label in enumerate(classes):\n",
    "        X_train_c = X_train[y_train==label]\n",
    "        priors[idx] = prior(X_train_c, m)\n",
    "        likelihoods[idx, :] = likelihood(X_train_c)\n",
    "    return priors, likelihoods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X_test, priors, likelihoods, classes):\n",
    "    return np.log(priors) + X_test @ np.log(likelihoods.T) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Let's use them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "priors, likelihoods = fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.unique(y_test)\n",
    "yhat = predict(X_test, priors, likelihoods, classes)\n",
    "yhat = np.argmax(yhat, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9168994413407822\n",
      "=========Average precision score=======\n",
      "Class 0 score:  0.9152047938418233\n",
      "Class 1 score:  0.9069918620723723\n",
      "Class 2 score:  0.8429395016564877\n",
      "Class 3 score:  0.7277310085946386\n",
      "=========Classification report=======\n",
      "Report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.95      0.95       389\n",
      "           1       0.94      0.96      0.95       394\n",
      "           2       0.87      0.95      0.91       398\n",
      "           3       0.92      0.74      0.82       251\n",
      "\n",
      "    accuracy                           0.92      1432\n",
      "   macro avg       0.92      0.90      0.91      1432\n",
      "weighted avg       0.92      0.92      0.92      1432\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import average_precision_score, classification_report\n",
    "\n",
    "n_classes = len(np.unique(y_test))\n",
    "\n",
    "print(\"Accuracy: \", np.sum(yhat == y_test)/len(y_test))\n",
    "\n",
    "print(\"=========Average precision score=======\")\n",
    "y_test_binarized = label_binarize(y_test, classes=[0, 1, 2, 3])\n",
    "yhat_binarized = label_binarize(yhat, classes=[0, 1, 2, 3])\n",
    "\n",
    "for i in range(n_classes):\n",
    "    class_score = average_precision_score(y_test_binarized[:, i], yhat_binarized[:, i])\n",
    "    print(f\"Class {i} score: \", class_score)\n",
    "    \n",
    "print(\"=========Classification report=======\")\n",
    "print(\"Report: \", classification_report(y_test, yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(32.99999999999999, 0.5, 'predicted')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#use confusion matrix\n",
    "mat = confusion_matrix(y_test, yhat)\n",
    "\n",
    "import seaborn as sns\n",
    "sns.heatmap(mat.T, annot=True, fmt=\"d\",\n",
    "           xticklabels=train.target_names, yticklabels=train.target_names)\n",
    "plt.xlabel('true')\n",
    "plt.ylabel('predicted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "yhat = model.predict(X_test)  ##later for checking score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comp.graphics\n"
     ]
    }
   ],
   "source": [
    "#fun thing you can do\n",
    "some_string = \"Programming is fun\"\n",
    "transformed = vectorizer.transform([some_string])\n",
    "transformed.shape\n",
    "\n",
    "prediction = model.predict(transformed)\n",
    "print(train.target_names[prediction[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========Average precision score=======\n",
      "Class 0 score:  0.9152047938418233\n",
      "Class 1 score:  0.9069918620723723\n",
      "Class 2 score:  0.8429395016564877\n",
      "Class 3 score:  0.7277310085946386\n",
      "=========Classification report=======\n",
      "Report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.95      0.95       389\n",
      "           1       0.94      0.96      0.95       394\n",
      "           2       0.87      0.95      0.91       398\n",
      "           3       0.92      0.74      0.82       251\n",
      "\n",
      "    accuracy                           0.92      1432\n",
      "   macro avg       0.92      0.90      0.91      1432\n",
      "weighted avg       0.92      0.92      0.92      1432\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=========Average precision score=======\")\n",
    "y_test_binarized = label_binarize(y_test, classes=[0, 1, 2, 3])\n",
    "yhat_binarized = label_binarize(yhat, classes=[0, 1, 2, 3])\n",
    "\n",
    "n_classes = len(np.unique(y_test))\n",
    "\n",
    "for i in range(n_classes):\n",
    "    class_score = average_precision_score(y_test_binarized[:, i], yhat_binarized[:, i])\n",
    "    print(f\"Class {i} score: \", class_score)\n",
    "\n",
    "print(\"=========Classification report=======\")\n",
    "print(\"Report: \", classification_report(y_test, yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to Use Naive Bayes\n",
    "\n",
    "Usually only as baseline!  Because naive Bayesian classifiers make such stringent assumptions about data, they will **generally NOT perform as well as a more complicated model.**\n",
    "That said, they have several advantages:\n",
    "\n",
    "- They are extremely fast for both training and prediction\n",
    "- They provide straightforward probabilistic prediction\n",
    "- They are often very easily interpretable\n",
    "- They have very few (if any) tunable parameters\n",
    "\n",
    "Naive Bayes classifiers tend to perform well only when your data is clearly separable or has high dimension.\n",
    "\n",
    "The reason for high dimension is because new dimensions usually add more information, thus data become more separable.  Thus, if you have really large dataset, try Naive Bayes and it may surprise you!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### === Task ===\n",
    "\n",
    "1) Learn about TfidfVectorizer and replace CountVectorizer with TfidfVectorizer (I have provided the explanation below.)\n",
    "2) Put Multinomial Naive Classification into a class that can transform the data, fit the model and do prediction.\n",
    "    - In the class, allow users to choose whether to use CountVectorizer or TfidfVectorizer to transform the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TfidfVectorizer\n",
    "\n",
    "Recall that in Naive Multinomial Classification, we want our features to be represented as frequency.  Here, we shall go beyond one more step, i.e., after counting the number of words, we shall perform a normalization process called TF-IDF which focuses on **cutting very frequent words which tend to be less meaningful information like \"the\", \"a\", \"is\".**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.91892665, 0.        , 0.39442846],\n",
       "       [0.84080197, 0.54134281, 0.        ],\n",
       "       [0.39706158, 0.34085938, 0.85214845]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "#imagine that we already have a frequency features.  We can perform normalization\n",
    "#as a follow up\n",
    "#here we got n=3, and m=2\n",
    "counts = [[3, 0, 1],\n",
    "          [2, 1, 0],\n",
    "          [3, 2, 5]]\n",
    "transformer = TfidfTransformer()\n",
    "transformer.fit_transform(counts).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how it works underhood:\n",
    "\n",
    "The formula is\n",
    "\n",
    "$$ \\text{TF-IDF} =  \\text{TF} * \\text{IDF} $$\n",
    "\n",
    "where TF is \n",
    "\n",
    "$$ \\text{TF}_t = \\frac{\\text{Count of words t in that document}}{\\text{Total count of words in that document}}$$\n",
    "\n",
    "Thus TF = \n",
    "\n",
    "| | 1st word  | 2nd word   | 3rd word |\n",
    "|---:|:-------------|:-----------|:-----------|\n",
    "| doc1 | 3/4 = 0.75  | 0     |  1/4 = 0.25 |\n",
    "| doc2 | 2/3 = 0.66  | 1/3 = 0.33    |  0 |\n",
    "| doc3 | 3/10 = 0.33  | 2/10 = 0.20    |  5/10 = 0.5 |\n",
    "\n",
    "and \n",
    "\n",
    "$$ \\text{IDF} = \\log\\left(\\frac{\\text{Number of documents}}{\\text{Number of documents containing that word}}\\right) + 1$$\n",
    "\n",
    "*Note:  We plus one so that super frequent words will not be ignored entirely*\n",
    "\n",
    "Thus IDF = \n",
    "\n",
    "| | IDF  |    \n",
    "|---:|:-----------|\n",
    "| 1st word | $\\log$ 3/3 + 1 = 1 |\n",
    "| 2nd word | $\\log$ 3/2 + 1 = 1.4055  |\n",
    "| 3rd word | $\\log$ 3/2 + 1 = 1.4055  | \n",
    "\n",
    "*Notice that terms (i.e., 1st word) that appear frequently across documents will get low score.  By multiplying this IDF term with the frequency, it will scale the importance down.*\n",
    "\n",
    "Thus TF * IDF = \n",
    "\n",
    "| | 1st word  | 2nd word | 3rd word|    \n",
    "|---:|:-----------|:-----------|:-----------|\n",
    "| doc1 | 0.75 * 1 = 0.75  | 0 * 1.4055 = 0 | 0.25 * 1.4055 = 0.3514 |\n",
    "| doc2 | 0.66 * 1 = 0.66  | 0.33 * 1.4055 = 0.4685 | 0 * 1.4055 = 0   |\n",
    "| doc3 | 0.33 * 1 = 0.33  | 0.20 * 1.4055 = 0.2811 | 0.5 * 1.4055 =0.7027   |\n",
    "\n",
    "\n",
    "We need to further normalize each word using this formula (since each document has unequaled number of words):\n",
    "\n",
    "$$ norm(t_i) = \\frac{t_i}{\\sqrt{t_1^2 + t_2^2 + ....+t_n^2}} $$ \n",
    "\n",
    "Thus, normalized factor for each document is\n",
    "\n",
    "doc1 = $\\sqrt{0.75^2 + 0^2 + 0.3514^2} = 0.8282$\n",
    "\n",
    "doc2 = $\\sqrt{0.66^2 + 0.4685^2 + 0^2} = 0.8094$\n",
    "\n",
    "doc3 = $\\sqrt{0.33^2 + 0.281^2 + 0.7027^2} = 0.8256$\n",
    "\n",
    "\n",
    "Thus, normalized(TF * IDF) = \n",
    "\n",
    "| | 1st word  | 2nd word | 3rd word|    \n",
    "|---:|:-----------|:-----------|:-----------|\n",
    "| doc1 | 0.75 / 0.8282 = 0.9056 | 0 | 0.3514 / 0.8282 = 0.4243 |\n",
    "| doc2 | 0.66 / 0.8094 = 0.8154  | 0.4685 / 0.8094 = 0.5788 | 0   |\n",
    "| doc3 | 0.33 / 0.8256 = 0.3997  | 0.2811 / 0.8256 = 0.3405 | 0.7027 / 0.8256 = 0.8511 |\n",
    "\n",
    "**Note**\n",
    "- My numbers are not exactly the same due to float precisions\n",
    "- Note I am using TfidfTransformer.  You may want to use TfidfVectorizer which automatically accepts raw data (i.e., text data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.venv': pipenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "27768773b483d82a9b2b839e3fa80b1be5789db7fd78df4eedef2df266871616"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
